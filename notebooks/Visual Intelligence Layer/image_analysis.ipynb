{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVvRT8e38ANGNzbKNHmqks",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }  
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwin-yedte/visual-intelligence-travel-finance/blob/main/notebooks/Visual%20Intelligence%20Layer/image_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VLM INTELLIGENCE LAYER **- STEP 1: IMAGE ANALYSIS\n",
        "Comprehensive image validation, preprocessing, and CLIP embedding extraction\n",
        "\n",
        "Features:\n",
        "- Strict validation (size, format, integrity)\n",
        "- EXIF orientation fixing\n",
        "- Aspect ratio preservation\n",
        "- Color statistics extraction\n",
        "- Error handling with user-friendly messages"
      ],
      "metadata": {
        "id": "Nx8yo20Qd-6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================\n",
        "Step 1: CONFIGURATION\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "7WHrpzsLeEMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STEP 1: IMAGE ANALYSIS - VLM INTELLIGENCE LAYER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Centralized configuration for image analysis\"\"\"\n",
        "\n",
        "    # Model configuration\n",
        "    CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "    EMBEDDING_DIMENSION = 512\n",
        "\n",
        "    # Image validation\n",
        "    TARGET_IMAGE_SIZE = (224, 224)\n",
        "    MAX_IMAGE_SIZE_MB = 10.0\n",
        "    SUPPORTED_FORMATS = ['jpg', 'jpeg', 'png']\n",
        "    MIN_DIMENSION = 100\n",
        "    MAX_DIMENSION = 4000\n",
        "\n",
        "    # Batch processing\n",
        "    MIN_IMAGES = 1\n",
        "    MAX_IMAGES = 5\n",
        "\n",
        "    # Color analysis\n",
        "    NUM_DOMINANT_COLORS = 3\n",
        "    COLOR_SAMPLE_SIZE = 10000\n",
        "\n",
        "    # Output\n",
        "    OUTPUT_FILE = \"step1_user_analysis.json\"\n",
        "\n",
        "print(\"Configuration loaded\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL2H7bKVeG4V",
        "outputId": "9ba687ad-9f57-43fb-ece8-588bd1215427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STEP 1: IMAGE ANALYSIS - VLM INTELLIGENCE LAYER\n",
            "================================================================================\n",
            "Configuration loaded\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================\n",
        "Step 2: IMAGE VALIDATION AND PREPROCESSING CLASS\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "47EAJigMeJCd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxXQJifMa7Ca"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "from typing import Dict, Any, List, Tuple\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class ImageValidator:\n",
        "    \"\"\"\n",
        "    Comprehensive image validation with detailed error reporting.\n",
        "    Returns user-friendly error messages for frontend display.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.max_size_mb = Config.MAX_IMAGE_SIZE_MB\n",
        "        self.supported_formats = Config.SUPPORTED_FORMATS\n",
        "        self.min_dimension = Config.MIN_DIMENSION\n",
        "        self.max_dimension = Config.MAX_DIMENSION\n",
        "\n",
        "    def validate_image(self, image_bytes: bytes, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Comprehensive validation with user-friendly error messages.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with validation results\n",
        "        \"\"\"\n",
        "\n",
        "        # Check 1: File size\n",
        "        size_mb = len(image_bytes) / (1024 * 1024)\n",
        "\n",
        "        if size_mb > self.max_size_mb:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': \"Image is too large. Maximum allowed is 10MB. Please compress or resize the image.\",\n",
        "                'error_code': 'FILE_TOO_LARGE',\n",
        "                'size_mb': size_mb,\n",
        "                'format': None,\n",
        "                'dimensions': None\n",
        "            }\n",
        "\n",
        "        if size_mb == 0:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': \"Image appears to be empty. Please select a valid image file.\",\n",
        "                'error_code': 'FILE_EMPTY',\n",
        "                'size_mb': 0,\n",
        "                'format': None,\n",
        "                'dimensions': None\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Attempt to open image\n",
        "            img = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Check 2: Format validation\n",
        "            img_format = img.format.lower() if img.format else 'unknown'\n",
        "\n",
        "            if img_format not in self.supported_formats:\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': \"Unsupported format. Please upload JPG or PNG images only.\",\n",
        "                    'error_code': 'UNSUPPORTED_FORMAT',\n",
        "                    'size_mb': size_mb,\n",
        "                    'format': img_format,\n",
        "                    'dimensions': None\n",
        "                }\n",
        "\n",
        "            # Check 3: Dimensions validation\n",
        "            width, height = img.size\n",
        "\n",
        "            if width < self.min_dimension or height < self.min_dimension:\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': \"Image is too small. Minimum size is 100x100 pixels.\",\n",
        "                    'error_code': 'IMAGE_TOO_SMALL',\n",
        "                    'size_mb': size_mb,\n",
        "                    'format': img_format,\n",
        "                    'dimensions': (width, height)\n",
        "                }\n",
        "\n",
        "            if width > self.max_dimension or height > self.max_dimension:\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': \"Image is too large. Maximum size is 4000x4000 pixels. Please resize.\",\n",
        "                    'error_code': 'IMAGE_TOO_LARGE',\n",
        "                    'size_mb': size_mb,\n",
        "                    'format': img_format,\n",
        "                    'dimensions': (width, height)\n",
        "                }\n",
        "\n",
        "            # Check 4: Image integrity\n",
        "            img.verify()\n",
        "\n",
        "            # Re-open after verify\n",
        "            img = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Try to load pixel data\n",
        "            try:\n",
        "                img.load()\n",
        "            except Exception as e:\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': \"Image appears to be corrupted. Please try a different image.\",\n",
        "                    'error_code': 'IMAGE_CORRUPTED',\n",
        "                    'size_mb': size_mb,\n",
        "                    'format': img_format,\n",
        "                    'dimensions': (width, height)\n",
        "                }\n",
        "\n",
        "            # All checks passed\n",
        "            return {\n",
        "                'valid': True,\n",
        "                'error': None,\n",
        "                'error_code': None,\n",
        "                'size_mb': size_mb,\n",
        "                'format': img_format,\n",
        "                'dimensions': (width, height)\n",
        "            }\n",
        "\n",
        "        except IOError:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': \"Unable to read file. The file may be corrupted or not a valid image.\",\n",
        "                'error_code': 'INVALID_IMAGE_FILE',\n",
        "                'size_mb': size_mb,\n",
        "                'format': None,\n",
        "                'dimensions': None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': \"Error processing image: \" + str(e),\n",
        "                'error_code': 'PROCESSING_ERROR',\n",
        "                'size_mb': size_mb,\n",
        "                'format': None,\n",
        "                'dimensions': None\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImagePreprocessor:\n",
        "    \"\"\"\n",
        "    CLIP-optimized image preprocessing with comprehensive transformations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.target_size = Config.TARGET_IMAGE_SIZE\n",
        "        self.validator = ImageValidator()\n",
        "\n",
        "    def preprocess_image(self, image_bytes: bytes) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Complete preprocessing pipeline for CLIP.\n",
        "\n",
        "        Returns:\n",
        "            PIL Image ready for CLIP (224x224, RGB)\n",
        "        \"\"\"\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "        # Fix orientation from EXIF\n",
        "        img = self._fix_orientation(img)\n",
        "\n",
        "        # Convert to RGB\n",
        "        if img.mode != 'RGB':\n",
        "            if img.mode == 'RGBA':\n",
        "                # Handle transparency\n",
        "                background = Image.new('RGB', img.size, (255, 255, 255))\n",
        "                background.paste(img, mask=img.split()[3] if len(img.split()) == 4 else None)\n",
        "                img = background\n",
        "            else:\n",
        "                img = img.convert('RGB')\n",
        "\n",
        "        # Resize with padding\n",
        "        img = self._resize_with_padding(img, self.target_size)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def _fix_orientation(self, img: Image.Image) -> Image.Image:\n",
        "        \"\"\"Auto-rotate image based on EXIF orientation tag.\"\"\"\n",
        "        try:\n",
        "            img = ImageOps.exif_transpose(img)\n",
        "        except Exception:\n",
        "            pass\n",
        "        return img\n",
        "\n",
        "    def _resize_with_padding(self, img: Image.Image, target_size: Tuple[int, int]) -> Image.Image:\n",
        "        \"\"\"Resize maintaining aspect ratio, add white padding.\"\"\"\n",
        "\n",
        "        # Calculate aspect ratios\n",
        "        img_ratio = img.width / img.height\n",
        "        target_ratio = target_size[0] / target_size[1]\n",
        "\n",
        "        # Determine new dimensions\n",
        "        if img_ratio > target_ratio:\n",
        "            new_width = target_size[0]\n",
        "            new_height = int(new_width / img_ratio)\n",
        "        else:\n",
        "            new_height = target_size[1]\n",
        "            new_width = int(new_height * img_ratio)\n",
        "\n",
        "        # Resize using LANCZOS filter\n",
        "        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Create white canvas\n",
        "        canvas = Image.new('RGB', target_size, (255, 255, 255))\n",
        "\n",
        "        # Center image\n",
        "        offset_x = (target_size[0] - new_width) // 2\n",
        "        offset_y = (target_size[1] - new_height) // 2\n",
        "        canvas.paste(img, (offset_x, offset_y))\n",
        "\n",
        "        return canvas\n",
        "\n",
        "    def extract_color_statistics(self, img: Image.Image) -> Dict[str, Any]:\n",
        "        \"\"\"Extract color features for analysis.\"\"\"\n",
        "\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        return {\n",
        "            'dominant_colors': self._get_dominant_colors(img_array),\n",
        "            'brightness': float(np.mean(img_array)),\n",
        "            'color_variance': float(np.var(img_array))\n",
        "        }\n",
        "\n",
        "    def _get_dominant_colors(self, img_array: np.ndarray) -> List[List[int]]:\n",
        "        \"\"\"Use K-means clustering to find dominant colors.\"\"\"\n",
        "\n",
        "        # Reshape to list of pixels\n",
        "        pixels = img_array.reshape(-1, 3)\n",
        "\n",
        "        # Sample if too many pixels\n",
        "        if len(pixels) > Config.COLOR_SAMPLE_SIZE:\n",
        "            indices = np.random.choice(len(pixels), Config.COLOR_SAMPLE_SIZE, replace=False)\n",
        "            pixels = pixels[indices]\n",
        "\n",
        "        # K-means clustering\n",
        "        kmeans = KMeans(n_clusters=Config.NUM_DOMINANT_COLORS, random_state=42, n_init=10)\n",
        "        kmeans.fit(pixels)\n",
        "\n",
        "        # Get cluster centers and counts\n",
        "        colors = kmeans.cluster_centers_.astype(int)\n",
        "        counts = np.bincount(kmeans.labels_)\n",
        "\n",
        "        # Sort by frequency\n",
        "        sorted_indices = np.argsort(-counts)\n",
        "        dominant_colors = colors[sorted_indices].tolist()\n",
        "\n",
        "        return dominant_colors\n",
        "\n",
        "\n",
        "print(\"Validation and Preprocessing classes loaded\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXHhm_xieZPd",
        "outputId": "5cd271b2-8326-4487-b69d-3bc9c540d919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation and Preprocessing classes loaded\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================\n",
        "Step 3: INITIALIZE PREPROCESSOR\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "lfJ6yt-vehyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nInitializing preprocessor...\")\n",
        "preprocessor = ImagePreprocessor()\n",
        "print(\"Preprocessor ready\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqcE-42cek9V",
        "outputId": "d9d09e10-7eaf-403e-86bd-c935aedb6b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initializing preprocessor...\n",
            "Preprocessor ready\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================\n",
        "Step 4: LOAD CLIP MODEL\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "-sXtduMXeO-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING CLIP MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device: \" + device)\n",
        "\n",
        "model = CLIPModel.from_pretrained(Config.CLIP_MODEL_NAME)\n",
        "processor = CLIPProcessor.from_pretrained(Config.CLIP_MODEL_NAME)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded: \" + Config.CLIP_MODEL_NAME)\n",
        "print(\"Embedding dimension: \" + str(Config.EMBEDDING_DIMENSION))\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791,
          "referenced_widgets": [
            "d4b7461c459049e39c6b46dbb79972a3",
            "db96db35dae04093a722d968a94d1b3e",
            "21ec9af77f8848f8b2261351963d1e91",
            "43cfe1fd8e9147e19e2995898741b225",
            "0bf1b964dd9341ae9d5a1742f62951cf",
            "83cab07f276b4713bda1b3381eecee1b",
            "023b99ce260b4047acb859fc8600d4e3",
            "38770dc1b5f54a1ea5412fb7c53851a6",
            "99c7563e0fb94e1fa53b6a2b1d769276",
            "eb4dd3b0c585481fa31d4b90e6395d48",
            "b32ff748d8314b23b8d00bc98a87dd42",
            "24b570b8a38540219fa9f556d4c9b3fd",
            "fbb33f393489445fb7c6183268d14e08",
            "a1f2a9e6daf644c7b550e0420f742f6e",
            "ed4fcbacd8fb4800bb70f1de8e69cc14",
            "1647387c7a484e6ca71565d40a604012",
            "f1f1b09756894bd2a0b79fe14999259c",
            "8bb7c05475494de286e17681ffca2fea",
            "e9e924d5c8a44645a196b6b4a9178736",
            "52531de07f984d8aa3c9d407ef49ac5d",
            "13a1a38d94f7491198526e6946db1abf",
            "86af3c8080b149529143e246500d74f3",
            "84076fbf871b47779667423161c8f5b2",
            "6ffed42e14864607846d8c18dd255068",
            "77f834e2a5cf4f28b2adfcf50044b86c",
            "d0cc5631767a4a4ab1b74872efef05e4",
            "61985fbc4d834855962834fadf38f01c",
            "b0b983894a174abea431c560a5303fc4",
            "3dd446db13f0479cb2387ac62db94048",
            "f81e40d7c95a478097d79805a56cc106",
            "909b663f2f494666a5a41fe63a33e759",
            "2423c07a55f2444c9937583d151ddcc3",
            "5b958750e6cb4f43bdcdeda0239ec404",
            "f611a2220ff64304b63c7f0c4ca6c7ef",
            "3d03fcddb3844ac6bc6f314f232409b7",
            "61f95a4d23814e9f88759aced56ac814",
            "591a40454afa468ea7a2c4d79c4cff4d",
            "547cad5186f94ad3a854aba960fde887",
            "0f8d6c60b808453785be85658ec1bee3",
            "09eb0a3b19154efeaf956b82b56f4f0b",
            "f4d832f841c64430b4d1e21348d80413",
            "498b4c0e021b40d28b12f5a885a1308b",
            "75894b0964404b68ae57b91bade5292f",
            "594164fc876940c7b5b0a56971ae104c",
            "35c5246755fc49dfafbe490a7bdf9f75",
            "ddbb43f9e8b04343a8d0d562f3da759c",
            "be22d47e5b2d453dbe3bbb31da998ee2",
            "d29969c02c6c41f4938910067fa50546",
            "6d4a9b7c65884efda0826f2ca75b2fb2",
            "02da109d7a0c4024ac593a3e2439babf",
            "0b069e2935f54c039d33e61362968d71",
            "1e572713f96d4abc851faa525fb7e9da",
            "7697c7fdfacf442fbcc6d839c7aa34d6",
            "72044e50164b41a5822ba6e4476834b7",
            "6d2bcbcfebaf47d79a5c4c1ac9e88b99",
            "e311dd1b29664bf4b14d1f30d1068f19",
            "664c25200fa54e149e7f14ae7fa26d6c",
            "7b80e61307eb4287ae75d1d95cb6b244",
            "6264b08bfa5b4376a5422c9f5b2954a9",
            "04c4fb8b819349f8b08697e5c3348585",
            "c0ebffb35f0642298768dc089deaa053",
            "1ae6ba553d474124b43123bba3f130a6",
            "ef554e383e6c49348b7587681991c55d",
            "35344cd0b3004a32a9a7920ef111ab7f",
            "a6f8e80cd4e246168cb4ca897ef766b7",
            "9102e6e7cd6142df9d0bad57428925c1",
            "6db115ac105f4ec6ae33d78309079888",
            "469402a80fd942a590d2fe49587070c2",
            "8909fa74d8f34716ac20a1af08681565",
            "a412936042bd4b8ab9c87e99f8f58c14",
            "27edc38cdab34205ae6a2a522bf45f6b",
            "923c7aac1e57458cbceafadcad3c2f3a",
            "d85b69087a5943dba3f996119c26472c",
            "0bdd1532ef5b42ecaf803a246b172161",
            "cef2f9f8ea3d445f9c66867242e2f2c5",
            "1141149d69984b8d82945e8db36d8ddd",
            "0161e2e5e4944106894946b30bb43522",
            "0dd2e94fb93f46a382ca7e06f2dafc3b",
            "520ba4d789154c92a47c0d022362bcd6",
            "7f75ad79851640eeada88748914d2c19",
            "8954d5e39b0d4087bcb46bb592d3ff11",
            "85d9c47d3a21450ba509cddc165affde",
            "7c55643db8ce4d0c919333d6e5ac6c98",
            "50b8a63655b54096ac8f8744ad049156",
            "d074b93862d64ac6a164faa85f0d272c",
            "e18aa22562d54123a72faf2a9045693c",
            "887fbcb292b346e99a2b4c99f9e7552a",
            "b8e00ae867a24f3c85d814ed87cbabd1",
            "082d85d82e0a42c8af9c8f76de3e0c01",
            "839298c92b654d049b1af824d87ab1ad",
            "2d80bb17b4e446b2ae05540bec4f0f44",
            "ec824a68d2eb4e23a56291bf7c1fff21",
            "46624526b18f4773b0eefc246b127e71",
            "6c7b48f8a293423dbdaae1f49cd14d34",
            "2b6c7a3db29e403e905494392a2b28c2",
            "520d324b3f1d4b8ebb170ad73f14bc35",
            "d64d1baae9d24f9fa37b75a3b1cb6461",
            "d717add1366347179c4b7fc9b59abc10",
            "9a9bbb20ed94459bbacfc7f4fbb1d400",
            "73f7058d20b2402d9ee50dcfa199cfd1",
            "34cdfa0dae6b4480a56b096933534a77",
            "dd13e565020c483f8fa23974e5d1c646",
            "d9641f54f93a435993c32ccc220a95e1",
            "827ae84c3c38420e9164054672c4ab39",
            "f72b7539d5744483bd9fcb25072ff19e",
            "cd50ee0f176b48aaae87883138755109",
            "fb677b13fcd74141a392d6038eac8ea1",
            "17c632e1765e4cc8981f41400bbd3a40",
            "6b5763b83f11474c8d56c727870096f8",
            "d3b85746cfbb403887484ba8d3044e85"
          ]
        },
        "id": "YsVx6xgMeOy2",
        "outputId": "98ceca5d-57db-49e1-b7ca-5c32c3a413b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING CLIP MODEL\n",
            "================================================================================\n",
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4b7461c459049e39c6b46dbb79972a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24b570b8a38540219fa9f556d4c9b3fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84076fbf871b47779667423161c8f5b2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f611a2220ff64304b63c7f0c4ca6c7ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIPModel LOAD REPORT from: openai/clip-vit-base-patch32\n",
            "Key                                  | Status     |  | \n",
            "-------------------------------------+------------+--+-\n",
            "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
            "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35c5246755fc49dfafbe490a7bdf9f75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e311dd1b29664bf4b14d1f30d1068f19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6db115ac105f4ec6ae33d78309079888"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dd2e94fb93f46a382ca7e06f2dafc3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "082d85d82e0a42c8af9c8f76de3e0c01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73f7058d20b2402d9ee50dcfa199cfd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: openai/clip-vit-base-patch32\n",
            "Embedding dimension: 512\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================\n",
        "Step 5: CLIP EMBEDDING EXTRACTION\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "udkzamhxesGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_clip_features(outputs):\n",
        "    \"\"\"Universal tensor extraction from CLIP outputs.\"\"\"\n",
        "    if torch.is_tensor(outputs):\n",
        "        return outputs\n",
        "\n",
        "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "        return outputs.pooler_output\n",
        "\n",
        "    if hasattr(outputs, 'image_embeds') and outputs.image_embeds is not None:\n",
        "        return outputs.image_embeds\n",
        "\n",
        "    if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:\n",
        "        return outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    raise ValueError(\"Cannot extract features from output type: \" + str(type(outputs)))\n",
        "\n",
        "\n",
        "def extract_clip_embedding(image: Image.Image) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Extract CLIP embedding from preprocessed image.\n",
        "\n",
        "    Returns:\n",
        "        Normalized 512-dim embedding as numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Process with CLIP\n",
        "        inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            image_features = extract_clip_features(outputs)\n",
        "            # L2 normalization\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return image_features.cpu().numpy()[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(\"CLIP embedding extraction failed: \" + str(e))\n",
        "\n",
        "\n",
        "print(\"CLIP extraction functions ready\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52tSkEYqer8V",
        "outputId": "5b54331a-e652-48d0-89e9-2b323057a87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP extraction functions ready\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================\n",
        "Step 6: MAIN ANALYSIS FUNCTION\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "cMkkvZeAewz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_user_images(files_dict: Dict[str, bytes]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Complete Step 1: Validate, preprocess, and extract embeddings.\n",
        "\n",
        "    Args:\n",
        "        files_dict: Dictionary mapping filenames to image bytes\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with status, results, embeddings, and summary\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYZING \" + str(len(files_dict)) + \" USER IMAGES\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Check batch size\n",
        "    if len(files_dict) < Config.MIN_IMAGES:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'error': \"Please upload at least \" + str(Config.MIN_IMAGES) + \" image(s).\",\n",
        "            'error_code': 'TOO_FEW_IMAGES'\n",
        "        }\n",
        "\n",
        "    if len(files_dict) > Config.MAX_IMAGES:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'error': \"Maximum \" + str(Config.MAX_IMAGES) + \" images allowed. You uploaded \" + str(len(files_dict)) + \".\",\n",
        "            'error_code': 'TOO_MANY_IMAGES'\n",
        "        }\n",
        "\n",
        "    validation_errors = []\n",
        "    results = []\n",
        "    embeddings = []\n",
        "\n",
        "    for filename, image_bytes in files_dict.items():\n",
        "        print(\"\\nProcessing: \" + filename)\n",
        "\n",
        "        # Step 1: Validate\n",
        "        validation = preprocessor.validator.validate_image(image_bytes, filename)\n",
        "\n",
        "        if not validation['valid']:\n",
        "            validation_errors.append({\n",
        "                'filename': filename,\n",
        "                'error': validation['error'],\n",
        "                'error_code': validation['error_code']\n",
        "            })\n",
        "            print(\"  Validation failed: \" + validation['error'])\n",
        "            continue\n",
        "\n",
        "        print(\"  Validated (\" + validation['format'].upper() + \", \" +\n",
        "              str(validation['dimensions'][0]) + \"x\" + str(validation['dimensions'][1]) +\n",
        "              \", \" + str(round(validation['size_mb'], 2)) + \"MB)\")\n",
        "\n",
        "        try:\n",
        "            # Step 2: Preprocess\n",
        "            processed_img = preprocessor.preprocess_image(image_bytes)\n",
        "            print(\"  Preprocessed to \" + str(processed_img.size))\n",
        "\n",
        "            # Step 3: Extract colors\n",
        "            color_stats = preprocessor.extract_color_statistics(processed_img)\n",
        "            print(\"  Color analysis complete\")\n",
        "\n",
        "            # Step 4: Extract CLIP embedding\n",
        "            embedding = extract_clip_embedding(processed_img)\n",
        "            print(\"  CLIP embedding extracted (\" + str(embedding.shape) + \")\")\n",
        "\n",
        "            # Store results\n",
        "            results.append({\n",
        "                'filename': filename,\n",
        "                'original_dimensions': validation['dimensions'],\n",
        "                'file_size_mb': validation['size_mb'],\n",
        "                'format': validation['format'],\n",
        "                'color_statistics': color_stats,\n",
        "                'embedding_shape': embedding.shape\n",
        "            })\n",
        "\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        except Exception as e:\n",
        "            validation_errors.append({\n",
        "                'filename': filename,\n",
        "                'error': \"Processing failed: \" + str(e),\n",
        "                'error_code': 'PROCESSING_FAILED'\n",
        "            })\n",
        "            print(\"  Processing error: \" + str(e))\n",
        "\n",
        "    # Check if we have any successful results\n",
        "    if len(embeddings) == 0:\n",
        "        return {\n",
        "            'status': 'error',\n",
        "            'error': 'All images failed validation or processing. Please check the error messages and try again.',\n",
        "            'error_code': 'ALL_IMAGES_FAILED',\n",
        "            'validation_errors': validation_errors\n",
        "        }\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    avg_brightness = np.mean([r['color_statistics']['brightness'] for r in results])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 1 COMPLETE\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Successfully processed: \" + str(len(embeddings)) + \"/\" + str(len(files_dict)) + \" images\")\n",
        "    if validation_errors:\n",
        "        print(\"Failed: \" + str(len(validation_errors)) + \" images\")\n",
        "    print(\"Average brightness: \" + str(round(avg_brightness, 1)))\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    return {\n",
        "        'status': 'success' if len(embeddings) > 0 else 'partial',\n",
        "        'num_uploaded': len(files_dict),\n",
        "        'num_processed': len(embeddings),\n",
        "        'num_failed': len(validation_errors),\n",
        "        'validation_errors': validation_errors,\n",
        "        'results': results,\n",
        "        'embeddings': embeddings,\n",
        "        'summary': {\n",
        "            'avg_brightness': float(avg_brightness),\n",
        "            'total_images': len(embeddings)\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "print(\"Main analysis function ready\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PNnt2b3ewo1",
        "outputId": "abeb29d9-f56d-47c2-fac7-62c7edbf1d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Main analysis function ready\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =================================================================\n",
        "Step 7: SAVE EMBEDDINGS FOR NEXT STEPS\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "l17UxrpMe8_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_step1_outputs(analysis_output: Dict) -> None:\n",
        "    \"\"\"\n",
        "    Save Step 1 outputs for use in Steps 2 and 3.\n",
        "\n",
        "    Saves:\n",
        "    - user_embeddings.npy: Individual embeddings\n",
        "    - step1_analysis.json: Full analysis results\n",
        "    \"\"\"\n",
        "\n",
        "    if analysis_output['status'] != 'success':\n",
        "        print(\"Cannot save - analysis did not succeed\")\n",
        "        return\n",
        "\n",
        "    # Save embeddings as numpy array\n",
        "    embeddings_array = np.array(analysis_output['embeddings'])\n",
        "    np.save('/content/user_embeddings.npy', embeddings_array)\n",
        "    print(\"\\nSaved embeddings: \" + str(embeddings_array.shape))\n",
        "\n",
        "    # Save full analysis\n",
        "    import json\n",
        "    analysis_json = {\n",
        "        'status': analysis_output['status'],\n",
        "        'num_processed': analysis_output['num_processed'],\n",
        "        'results': analysis_output['results'],\n",
        "        'summary': analysis_output['summary'],\n",
        "        'validation_errors': analysis_output['validation_errors']\n",
        "    }\n",
        "\n",
        "    with open('/content/' + Config.OUTPUT_FILE, 'w') as f:\n",
        "        json.dump(analysis_json, f, indent=2)\n",
        "\n",
        "    print(\"Saved analysis: \" + Config.OUTPUT_FILE)\n",
        "    print(\"\\nReady for Step 2: Destination Matching\")\n",
        "\n",
        "\n",
        "print(\"Save functions ready\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nSTEP 1 INITIALIZED - Ready to analyze images\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETWF8uzBe8z_",
        "outputId": "22dbf0f8-737d-43c3-daa2-12c360410669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Save functions ready\n",
            "================================================================================\n",
            "\n",
            "STEP 1 INITIALIZED - Ready to analyze images\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6o0bp5/wbCOzUNkTSK0yR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwin-yedte/visual-intelligence-travel-finance/blob/main/notebooks/Step_1_Image_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Image Analysis & Feature Extraction\n",
        "\n",
        "**Version**: 1.0  \n",
        "**Author**: Ashwin Kumar Y (2023AC05628)  \n",
        "**Project**: Visual Intelligence for Travel & Finance Optimization  \n",
        "**Institution**: BITS Pilani  \n",
        "**Date**: January 2026\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements:\n",
        "- Image preprocessing and validation\n",
        "- CLIP-based feature extraction (512-dim embeddings)\n",
        "- Zero-shot scene classification for Indian seashores\n",
        "- Batch processing for 1-5 images\n",
        "\n",
        "## Outputs\n",
        "- Scene classification scores\n",
        "- Visual descriptors\n",
        "- Color statistics\n",
        "- Dominant themes\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QqWFW3EbqGhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXcbJwsyp-ZE",
        "outputId": "f3814fbd-1c26-4dd8-a530-5c301a545181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "======================================================================\n",
            "Installation complete!\n",
            "======================================================================\n",
            "\n",
            " PyTorch version: 2.9.0+cpu\n",
            " Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SETUP & INSTALLATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Installing required packages...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q transformers torch torchvision pillow scikit-learn\n",
        "\n",
        "print(\"Installation complete!\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple\n",
        "import io\n",
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\n PyTorch version: {torch.__version__}\")\n",
        "print(f\" Device: {device}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\" GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuration **Constants**"
      ],
      "metadata": {
        "id": "fYK6MQ6Du9u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "CELL 3: CONFIGURATION CONSTANTS\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CONFIGURATION: Setting up system parameters\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    Centralized configuration for the image analysis system\n",
        "    \"\"\"\n",
        "\n",
        "    # Model configuration\n",
        "    CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "    EMBEDDING_DIMENSION = 512\n",
        "\n",
        "    # Image processing\n",
        "    TARGET_IMAGE_SIZE = (224, 224)\n",
        "    MAX_IMAGE_SIZE_MB = 10.0\n",
        "    SUPPORTED_FORMATS = ['jpg', 'jpeg', 'png']\n",
        "\n",
        "    # Batch processing\n",
        "    MIN_IMAGES = 1\n",
        "    MAX_IMAGES = 5\n",
        "\n",
        "    # Color analysis\n",
        "    NUM_DOMINANT_COLORS = 3\n",
        "    COLOR_SAMPLE_SIZE = 10000\n",
        "\n",
        "    # Scene classification\n",
        "    TOP_K_SCENES = 5\n",
        "    HIGH_CONFIDENCE_THRESHOLD = 0.75\n",
        "    MEDIUM_CONFIDENCE_THRESHOLD = 0.60\n",
        "    HIGH_CONFIDENCE_GAP = 0.15\n",
        "    MEDIUM_CONFIDENCE_GAP = 0.10\n",
        "\n",
        "    # Thresholds\n",
        "    BRIGHTNESS_BRIGHT_THRESHOLD = 150\n",
        "    BRIGHTNESS_MODERATE_THRESHOLD = 100\n",
        "    THEME_CONSISTENCY_THRESHOLD = 0.5\n",
        "\n",
        "    # Output\n",
        "    OUTPUT_JSON_FILE = \"step1_analysis_results.json\"\n",
        "    OUTPUT_VISUALIZATION_FILE = \"step1_visualization.png\"\n",
        "    VISUALIZATION_DPI = 150\n",
        "\n",
        "    # Metadata\n",
        "    VERSION = \"1.0.0\"\n",
        "    STEP_NAME = \"Step 1: Image Analysis and Feature Extraction\"\n",
        "    AUTHOR = \"Ashwin Kumar Y (2023AC05628)\"\n",
        "    PROJECT = \"Visual Intelligence for Travel and Finance Optimization\"\n",
        "\n",
        "    @classmethod\n",
        "    def display_config(cls):\n",
        "        \"\"\"Display current configuration\"\"\"\n",
        "        print(\"\\nCurrent Configuration:\")\n",
        "        print(f\"  Model: {cls.CLIP_MODEL_NAME}\")\n",
        "        print(f\"  Target Image Size: {cls.TARGET_IMAGE_SIZE}\")\n",
        "        print(f\"  Max Images per Batch: {cls.MAX_IMAGES}\")\n",
        "        print(f\"  Embedding Dimension: {cls.EMBEDDING_DIMENSION}\")\n",
        "        print(f\"  Supported Formats: {', '.join(cls.SUPPORTED_FORMATS)}\")\n",
        "\n",
        "# Display configuration\n",
        "Config.display_config()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONFIGURATION COMPLETE: System parameters set\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO3j4KJUu8FX",
        "outputId": "e62e3fde-3a40-4a55-b9a5-266342aba5d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CONFIGURATION: Setting up system parameters\n",
            "================================================================================\n",
            "\n",
            "Current Configuration:\n",
            "  Model: openai/clip-vit-base-patch32\n",
            "  Target Image Size: (224, 224)\n",
            "  Max Images per Batch: 5\n",
            "  Embedding Dimension: 512\n",
            "  Supported Formats: jpg, jpeg, png\n",
            "\n",
            "================================================================================\n",
            "CONFIGURATION COMPLETE: System parameters set\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROMPT **LIBRARY**"
      ],
      "metadata": {
        "id": "E4uvsQRnvvOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "================================================================================\n",
        "CELL 4: SEASHORE PROMPT LIBRARY\n",
        "================================================================================\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PROMPT LIBRARY: Loading scene classification prompts\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class SeashorePromptLibrary:\n",
        "    \"\"\"\n",
        "    Collection of specialized prompts for Indian seashore scene classification.\n",
        "    Organized by category for systematic theme extraction.\n",
        "    \"\"\"\n",
        "\n",
        "    # Primary seashore characteristics\n",
        "    PRIMARY_PROMPTS = [\n",
        "        \"a tropical beach with palm trees and golden sand\",\n",
        "        \"a pristine white sand beach with clear turquoise water\",\n",
        "        \"a rocky coastline with cliffs and crashing waves\",\n",
        "        \"a serene beach with calm waters and gentle waves\",\n",
        "        \"a beach with traditional fishing boats and nets\",\n",
        "        \"a secluded cove with crystal clear water\",\n",
        "        \"a sunset beach with orange and pink sky\",\n",
        "        \"a beach with water sports activities and equipment\",\n",
        "    ]\n",
        "\n",
        "    # Indian region-specific characteristics\n",
        "    INDIAN_REGIONAL_PROMPTS = [\n",
        "        \"a Goa style beach with colorful shacks and palm trees\",\n",
        "        \"a Kerala beach with coconut groves and backwaters\",\n",
        "        \"an Andaman island beach with coral reefs and pristine water\",\n",
        "        \"a Konkan coast beach with rocky cliffs and coconut trees\",\n",
        "        \"a beach with traditional fishing village and local culture\",\n",
        "        \"a beach near a temple or coastal religious site\",\n",
        "        \"a mangrove lined coastal area with dense vegetation\",\n",
        "        \"a Tamil Nadu beach with rocky shores\",\n",
        "    ]\n",
        "\n",
        "    # Activity and atmosphere\n",
        "    ACTIVITY_PROMPTS = [\n",
        "        \"a peaceful beach ideal for relaxation and meditation\",\n",
        "        \"an adventure beach with water sports and activities\",\n",
        "        \"a beach with beach parties and nightlife\",\n",
        "        \"a family friendly beach with safe shallow waters\",\n",
        "        \"a romantic beach with scenic sunset views\",\n",
        "        \"an offbeat secluded beach with minimal crowds\",\n",
        "    ]\n",
        "\n",
        "    # Visual aesthetics\n",
        "    AESTHETIC_PROMPTS = [\n",
        "        \"a beach with dramatic landscape and scenic views\",\n",
        "        \"a beach with clear blue sky and bright sunlight\",\n",
        "        \"a beach with golden hour lighting and warm tones\",\n",
        "        \"a beach with lush green vegetation and natural beauty\",\n",
        "        \"a beach with unique rock formations and natural features\",\n",
        "    ]\n",
        "\n",
        "    @classmethod\n",
        "    def get_all_prompts(cls) -> List[str]:\n",
        "        \"\"\"\n",
        "        Returns all prompts combined across categories.\n",
        "\n",
        "        Returns:\n",
        "            List of all prompt strings\n",
        "        \"\"\"\n",
        "        return (cls.PRIMARY_PROMPTS +\n",
        "                cls.INDIAN_REGIONAL_PROMPTS +\n",
        "                cls.ACTIVITY_PROMPTS +\n",
        "                cls.AESTHETIC_PROMPTS)\n",
        "\n",
        "    @classmethod\n",
        "    def get_prompt_categories(cls) -> Dict[str, List[str]]:\n",
        "        \"\"\"\n",
        "        Returns prompts organized by category.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping category names to prompt lists\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'primary': cls.PRIMARY_PROMPTS,\n",
        "            'regional': cls.INDIAN_REGIONAL_PROMPTS,\n",
        "            'activity': cls.ACTIVITY_PROMPTS,\n",
        "            'aesthetic': cls.AESTHETIC_PROMPTS\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def get_category_for_prompt(cls, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Identifies which category a prompt belongs to.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt string to categorize\n",
        "\n",
        "        Returns:\n",
        "            Category name or 'unknown'\n",
        "        \"\"\"\n",
        "        categories = cls.get_prompt_categories()\n",
        "        for category, prompts in categories.items():\n",
        "            if prompt in prompts:\n",
        "                return category\n",
        "        return 'unknown'\n",
        "\n",
        "    @classmethod\n",
        "    def display_statistics(cls):\n",
        "        \"\"\"Display statistics about the prompt library\"\"\"\n",
        "        categories = cls.get_prompt_categories()\n",
        "        total = len(cls.get_all_prompts())\n",
        "\n",
        "        print(\"\\nPrompt Library Statistics:\")\n",
        "        print(f\"  Total prompts: {total}\")\n",
        "        for category, prompts in categories.items():\n",
        "            print(f\"    {category.capitalize()}: {len(prompts)} prompts\")\n",
        "\n",
        "\n",
        "# Initialize and display statistics\n",
        "SeashorePromptLibrary.display_statistics()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROMPT LIBRARY COMPLETE: 27 specialized prompts loaded\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2AMUIMnsQMZ",
        "outputId": "36e85436-ebb6-42a8-8f1d-bf43a56d7a7d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PROMPT LIBRARY: Loading scene classification prompts\n",
            "================================================================================\n",
            "\n",
            "Prompt Library Statistics:\n",
            "  Total prompts: 27\n",
            "    Primary: 8 prompts\n",
            "    Regional: 8 prompts\n",
            "    Activity: 6 prompts\n",
            "    Aesthetic: 5 prompts\n",
            "\n",
            "================================================================================\n",
            "PROMPT LIBRARY COMPLETE: 27 specialized prompts loaded\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================================\n",
        " IMAGE PREPROCESSOR CLASS\n",
        "================================================================================\n",
        "Purpose: Handles image validation, preprocessing, and feature extraction\n",
        "         before feeding to the CLIP model.\n",
        "================================================================================================================================\n"
      ],
      "metadata": {
        "id": "59mkPkruwiWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"IMAGE PREPROCESSOR: Initializing image processing pipeline\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from typing import Any # Added this import\n",
        "\n",
        "class ImagePreprocessor:\n",
        "    \"\"\"\n",
        "    Comprehensive image preprocessing pipeline for beach/seashore images.\n",
        "\n",
        "    This class handles the complete preprocessing workflow including:\n",
        "    - Image validation (format, size, integrity)\n",
        "    - Image standardization (resize, orientation correction)\n",
        "    - Color statistics extraction\n",
        "    - Dominant color identification using K-means clustering\n",
        "\n",
        "    The preprocessor ensures all images meet the requirements for CLIP model\n",
        "    input while maintaining aspect ratio and extracting useful metadata.\n",
        "\n",
        "    Attributes:\n",
        "        target_size (Tuple[int, int]): Target dimensions for resized images\n",
        "        max_size_mb (float): Maximum allowed file size in megabytes\n",
        "        supported_formats (List[str]): List of supported image formats\n",
        "\n",
        "    Example:\n",
        "        >>> preprocessor = ImagePreprocessor()\n",
        "        >>> validation = preprocessor.validate_image(image_bytes, \"beach.jpg\")\n",
        "        >>> if validation['valid']:\n",
        "        >>>     processed_img = preprocessor.preprocess_image(image_bytes)\n",
        "        >>>     color_stats = preprocessor.extract_color_statistics(processed_img)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 target_size: Tuple[int, int] = None,\n",
        "                 max_size_mb: float = None,\n",
        "                 supported_formats: List[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize the ImagePreprocessor with configuration parameters.\n",
        "\n",
        "        Args:\n",
        "            target_size: Target dimensions (width, height) for resized images.\n",
        "                        Defaults to Config.TARGET_IMAGE_SIZE if not provided.\n",
        "            max_size_mb: Maximum file size in MB. Defaults to Config.MAX_IMAGE_SIZE_MB.\n",
        "            supported_formats: List of supported formats. Defaults to Config.SUPPORTED_FORMATS.\n",
        "\n",
        "        Note:\n",
        "            Using default values from Config class ensures consistency across the system.\n",
        "        \"\"\"\n",
        "        self.target_size = target_size or Config.TARGET_IMAGE_SIZE\n",
        "        self.max_size_mb = max_size_mb or Config.MAX_IMAGE_SIZE_MB\n",
        "        self.supported_formats = supported_formats or Config.SUPPORTED_FORMATS\n",
        "\n",
        "        print(f\"ImagePreprocessor initialized\")\n",
        "        print(f\"  Target size: {self.target_size}\")\n",
        "        print(f\"  Max file size: {self.max_size_mb} MB\")\n",
        "        print(f\"  Supported formats: {', '.join(self.supported_formats)}\")\n",
        "\n",
        "    def validate_image(self, image_bytes: bytes, filename: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate image file before processing.\n",
        "\n",
        "        Performs the following checks:\n",
        "        1. File size validation (must be under max_size_mb)\n",
        "        2. Format validation (must be in supported_formats)\n",
        "        3. Integrity check (file must be readable and not corrupted)\n",
        "        4. Dimension extraction\n",
        "\n",
        "        Args:\n",
        "            image_bytes: Raw image data as bytes\n",
        "            filename: Original filename (used for error messages)\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing validation results with keys:\n",
        "                - valid (bool): Whether image passed all validation checks\n",
        "                - error (str or None): Error message if validation failed\n",
        "                - size_mb (float): File size in megabytes\n",
        "                - format (str or None): Image format (jpg, png, etc.)\n",
        "                - dimensions (Tuple[int, int] or None): Image dimensions (width, height)\n",
        "\n",
        "        Example:\n",
        "            >>> result = preprocessor.validate_image(img_bytes, \"beach.jpg\")\n",
        "            >>> if result['valid']:\n",
        "            >>>     print(f\"Valid image: {result['dimensions']}\")\n",
        "            >>> else:\n",
        "            >>>     print(f\"Invalid: {result['error']}\")\n",
        "        \"\"\"\n",
        "        # Calculate file size in megabytes\n",
        "        size_mb = len(image_bytes) / (1024 * 1024)\n",
        "\n",
        "        # Check 1: File size validation\n",
        "        if size_mb > self.max_size_mb:\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': f\"File size {size_mb:.2f}MB exceeds maximum {self.max_size_mb}MB\",\n",
        "                'size_mb': size_mb,\n",
        "                'format': None,\n",
        "                'dimensions': None\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            # Attempt to open image\n",
        "            img = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # Extract format\n",
        "            img_format = img.format.lower() if img.format else 'unknown'\n",
        "\n",
        "            # Check 2: Format validation\n",
        "            if img_format not in self.supported_formats:\n",
        "                return {\n",
        "                    'valid': False,\n",
        "                    'error': f\"Unsupported format '{img_format}'. Supported: {', '.join(self.supported_formats)}\",\n",
        "                    'size_mb': size_mb,\n",
        "                    'format': img_format,\n",
        "                    'dimensions': None\n",
        "                }\n",
        "\n",
        "            # Check 3: Integrity validation\n",
        "            # verify() checks if file is readable and not corrupted\n",
        "            img.verify()\n",
        "\n",
        "            # Re-open image after verify() (verify closes the file handle)\n",
        "            img = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "            # All checks passed\n",
        "            return {\n",
        "                'valid': True,\n",
        "                'error': None,\n",
        "                'size_mb': size_mb,\n",
        "                'format': img_format,\n",
        "                'dimensions': img.size  # (width, height)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any errors during validation\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'error': f\"Image validation failed: {str(e)}\",\n",
        "                'size_mb': size_mb,\n",
        "                'format': None,\n",
        "                'dimensions': None\n",
        "            }\n",
        "\n",
        "    def preprocess_image(self, image_bytes: bytes) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Preprocess image for CLIP model input.\n",
        "\n",
        "        Performs the following operations in sequence:\n",
        "        1. Load image from bytes\n",
        "        2. Fix orientation using EXIF data (auto-rotate if needed)\n",
        "        3. Convert to RGB color mode (handles grayscale, RGBA, etc.)\n",
        "        4. Resize to target size while maintaining aspect ratio\n",
        "        5. Add white padding if needed to match target dimensions\n",
        "\n",
        "        Args:\n",
        "            image_bytes: Raw image data as bytes\n",
        "\n",
        "        Returns:\n",
        "            Preprocessed PIL Image object in RGB mode with target dimensions\n",
        "\n",
        "        Note:\n",
        "            The resizing maintains aspect ratio to prevent distortion.\n",
        "            White padding is added to fill remaining space.\n",
        "\n",
        "        Example:\n",
        "            >>> processed_img = preprocessor.preprocess_image(img_bytes)\n",
        "            >>> print(processed_img.size)  # Should be target_size\n",
        "            >>> print(processed_img.mode)  # Should be 'RGB'\n",
        "        \"\"\"\n",
        "        # Step 1: Load image\n",
        "        img = Image.open(io.BytesIO(image_bytes))\n",
        "\n",
        "        # Step 2: Fix orientation based on EXIF data\n",
        "        # Many smartphone photos have rotation info in EXIF metadata\n",
        "        img = self._fix_orientation(img)\n",
        "\n",
        "        # Step 3: Convert to RGB\n",
        "        # CLIP requires RGB input, so convert if image is grayscale, RGBA, etc.\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "\n",
        "        # Step 4: Resize with padding to maintain aspect ratio\n",
        "        img = self._resize_with_padding(img, self.target_size)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def _fix_orientation(self, img: Image.Image) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Fix image orientation based on EXIF metadata.\n",
        "\n",
        "        Many images (especially from smartphones) contain EXIF orientation data\n",
        "        that specifies how the image should be rotated for proper display.\n",
        "        This method reads that data and auto-rotates the image if needed.\n",
        "\n",
        "        Args:\n",
        "            img: PIL Image object\n",
        "\n",
        "        Returns:\n",
        "            Image with corrected orientation\n",
        "\n",
        "        Note:\n",
        "            If EXIF processing fails (e.g., no EXIF data), returns original image.\n",
        "            This is a safe operation that won't break the pipeline.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from PIL import ImageOps\n",
        "            # exif_transpose automatically handles all EXIF rotation cases\n",
        "            img = ImageOps.exif_transpose(img)\n",
        "        except Exception:\n",
        "            # If EXIF processing fails, continue with original image\n",
        "            # This can happen with images that don't have EXIF data\n",
        "            pass\n",
        "        return img\n",
        "\n",
        "    def _resize_with_padding(self, img: Image.Image, target_size: Tuple[int, int]) -> Image.Image:\n",
        "        \"\"\"\n",
        "        Resize image to target size while maintaining aspect ratio.\n",
        "\n",
        "        Algorithm:\n",
        "        1. Calculate aspect ratio of input image and target size\n",
        "        2. Determine scaling factor to fit image within target dimensions\n",
        "        3. Resize image using high-quality LANCZOS resampling\n",
        "        4. Create white canvas with target dimensions\n",
        "        5. Paste resized image in center of canvas\n",
        "\n",
        "        This approach ensures:\n",
        "        - No distortion (aspect ratio preserved)\n",
        "        - No cropping (entire image visible)\n",
        "        - Consistent output dimensions\n",
        "\n",
        "        Args:\n",
        "            img: PIL Image to resize\n",
        "            target_size: Desired output size (width, height)\n",
        "\n",
        "        Returns:\n",
        "            Resized image with white padding if needed\n",
        "\n",
        "        Example:\n",
        "            Input:  1920x1080 image, target 224x224\n",
        "            Output: 224x224 image with 224x126 content and white bars top/bottom\n",
        "        \"\"\"\n",
        "        # Calculate aspect ratios\n",
        "        img_aspect_ratio = img.width / img.height\n",
        "        target_aspect_ratio = target_size[0] / target_size[1]\n",
        "\n",
        "        # Determine new dimensions based on aspect ratio\n",
        "        if img_aspect_ratio > target_aspect_ratio:\n",
        "            # Image is wider than target - fit to width\n",
        "            new_width = target_size[0]\n",
        "            new_height = int(new_width / img_aspect_ratio)\n",
        "        else:\n",
        "            # Image is taller than target - fit to height\n",
        "            new_height = target_size[1]\n",
        "            new_width = int(new_height * img_aspect_ratio)\n",
        "\n",
        "        # Resize using high-quality LANCZOS filter\n",
        "        # LANCZOS provides best quality for downsampling\n",
        "        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "        # Create white canvas with target size\n",
        "        canvas = Image.new('RGB', target_size, (255, 255, 255))\n",
        "\n",
        "        # Calculate position to center the image\n",
        "        offset_x = (target_size[0] - new_width) // 2\n",
        "        offset_y = (target_size[1] - new_height) // 2\n",
        "\n",
        "        # Paste resized image onto canvas\n",
        "        canvas.paste(img, (offset_x, offset_y))\n",
        "\n",
        "        return canvas\n",
        "\n",
        "    def extract_color_statistics(self, img: Image.Image) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Extract comprehensive color statistics from image.\n",
        "\n",
        "        Computes:\n",
        "        1. Dominant colors using K-means clustering\n",
        "        2. Overall brightness (mean pixel value)\n",
        "        3. Color variance (measure of color diversity)\n",
        "\n",
        "        These statistics help characterize the visual appearance:\n",
        "        - Dominant colors indicate scene type (blue=water, green=vegetation, etc.)\n",
        "        - Brightness helps identify time of day\n",
        "        - Variance indicates visual complexity\n",
        "\n",
        "        Args:\n",
        "            img: PIL Image object\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with keys:\n",
        "                - dominant_colors: List of RGB triplets for top N colors\n",
        "                - brightness: Mean pixel value (0-255)\n",
        "                - color_variance: Variance in pixel values\n",
        "\n",
        "        Example:\n",
        "            >>> stats = preprocessor.extract_color_statistics(img)\n",
        "            >>> print(f\"Top color: RGB{stats['dominant_colors'][0]}\")\n",
        "            >>> print(f\"Brightness: {stats['brightness']:.1f}\")\n",
        "        \"\"\"\n",
        "        # Convert PIL Image to numpy array for processing\n",
        "        img_array = np.array(img)\n",
        "\n",
        "        return {\n",
        "            'dominant_colors': self._get_dominant_colors(img_array),\n",
        "            'brightness': float(np.mean(img_array)),\n",
        "            'color_variance': float(np.var(img_array))\n",
        "        }\n",
        "\n",
        "    def _get_dominant_colors(self, img_array: np.ndarray, n_colors: int = None) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Identify dominant colors in image using K-means clustering.\n",
        "\n",
        "        Algorithm:\n",
        "        1. Reshape image array to list of pixels (each pixel = RGB triplet)\n",
        "        2. Sample pixels if image is large (for performance)\n",
        "        3. Apply K-means clustering to group similar colors\n",
        "        4. Sort clusters by size (frequency)\n",
        "        5. Return cluster centers as dominant colors\n",
        "\n",
        "        Args:\n",
        "            img_array: Image as numpy array (height, width, 3)\n",
        "            n_colors: Number of dominant colors to extract\n",
        "\n",
        "        Returns:\n",
        "            List of RGB triplets sorted by dominance\n",
        "            Example: [[135, 206, 235], [255, 245, 200], [34, 139, 34]]\n",
        "                     (sky blue,       sand beige,      vegetation green)\n",
        "\n",
        "        Note:\n",
        "            For performance, limits analysis to 10,000 random pixels\n",
        "            if image is larger. This provides good approximation while\n",
        "            maintaining fast execution.\n",
        "        \"\"\"\n",
        "        n_colors = n_colors or Config.NUM_DOMINANT_COLORS\n",
        "\n",
        "        # Reshape from (height, width, 3) to (num_pixels, 3)\n",
        "        pixels = img_array.reshape(-1, 3)\n",
        "\n",
        "        # Sample pixels if too many (for performance)\n",
        "        if len(pixels) > Config.COLOR_SAMPLE_SIZE:\n",
        "            # Randomly sample pixels\n",
        "            indices = np.random.choice(len(pixels), Config.COLOR_SAMPLE_SIZE, replace=False)\n",
        "            pixels = pixels[indices]\n",
        "\n",
        "        # Apply K-means clustering\n",
        "        # n_init=10 means algorithm runs 10 times with different initializations\n",
        "        # and picks best result (more stable than default)\n",
        "        kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)\n",
        "        kmeans.fit(pixels)\n",
        "\n",
        "        # Get cluster centers (these are our dominant colors)\n",
        "        colors = kmeans.cluster_centers_.astype(int)\n",
        "\n",
        "        # Get cluster assignments for each pixel\n",
        "        labels = kmeans.labels_\n",
        "\n",
        "        # Count pixels in each cluster\n",
        "        counts = np.bincount(labels)\n",
        "\n",
        "        # Sort colors by frequency (most common first)\n",
        "        sorted_indices = np.argsort(-counts)  # Negative for descending order\n",
        "        dominant_colors = colors[sorted_indices].tolist()\n",
        "\n",
        "        return dominant_colors\n",
        "\n",
        "\n",
        "# Initialize the preprocessor\n",
        "print(\"\\nInitializing ImagePreprocessor...\")\n",
        "preprocessor = ImagePreprocessor()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IMAGE PREPROCESSOR COMPLETE: Ready to process images\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fatPrzq3wmSi",
        "outputId": "1fea99f7-e47c-4edd-ebbc-6f19966835cc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "IMAGE PREPROCESSOR: Initializing image processing pipeline\n",
            "================================================================================\n",
            "\n",
            "Initializing ImagePreprocessor...\n",
            "ImagePreprocessor initialized\n",
            "  Target size: (224, 224)\n",
            "  Max file size: 10.0 MB\n",
            "  Supported formats: jpg, jpeg, png\n",
            "\n",
            "================================================================================\n",
            "IMAGE PREPROCESSOR COMPLETE: Ready to process images\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}
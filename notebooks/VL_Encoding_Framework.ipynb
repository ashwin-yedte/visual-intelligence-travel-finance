{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },    
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**VL ENCODING FRAMEWORK** - WITH COMPREHENSIVE IMAGE PREPROCESSING\n",
        "Visual-Language Encoding Infrastructure for Indian Travel Destinations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "COMPLETE FEATURE SET:\n",
        "1.  CLIP tensor extraction bug\n",
        "2. Comprehensive image preprocessing (EXIF, RGB, aspect ratio)\n",
        "3. Image validation and quality checks\n",
        "4. Prompt validation (normalization, token limits)\n",
        "5. Quality reports and statistics\n",
        "6. Error recovery and logging\n",
        "\n",
        "PREPROCESSING PIPELINES:\n",
        "- IMAGE: EXIF orientation → RGB conversion → Aspect ratio → Padding → Validation\n",
        "- PROMPT: Token validation → Duplicate detection → Quality metrics\n",
        "\n",
        "OBJECTIVES:\n",
        "1. Pre-compute CLIP embeddings for all landmark images\n",
        "2. Extract semantic prompts using 2,200 CLIP prompt library\n",
        "3. Create searchable database for similarity matching\n",
        "4. Enable two-stage matching (visual + semantic)\n",
        "\n",
        "\n",
        "WORKFLOW:\n",
        "- Stage 1: Pre-compute prompt embeddings (one-time, 2,200 prompts)\n",
        "- Stage 2: Process each image (extract CLIP embedding + prompts)\n",
        "- Stage 3: Aggregate per destination\n",
        "- Stage 4: Create search indices"
      ],
      "metadata": {
        "id": "1rAhaa4qDLkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ZBdPB1n5e5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "INSTALL PACKAGES\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "lwlk_K2i5fJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch pillow tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PACKAGES INSTALLED\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0IsInVQEDQm",
        "outputId": "8839f62d-fed3-4751-ac08-3f7c091ae036"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PACKAGES INSTALLED\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "IMPORT PACKAGES\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "TP3tbv--gYOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import warnings\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "metadata": {
        "id": "IcOPRpFDgXIb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================================================================\n",
        "   SETUP AND MOUNT Google Drive   \n",
        "# ================================================================\n"
      ],
      "metadata": {
        "id": "4i1o_Y4eDW52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZkPBXp3CkxA",
        "outputId": "7475a9d0-0209-49cd-e0ce-2801820b3492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "VISUAL LANGUAGE ENCODING FRAMEWORK\n",
            "================================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directories created\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"VISUAL LANGUAGE ENCODING FRAMEWORK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/visual-intelligence-travel-finance'\n",
        "\n",
        "LANDMARKS_PATH = f'{BASE_PATH}/data/landmarks'\n",
        "METADATA_PATH = f'{LANDMARKS_PATH}/metadata.json'\n",
        "PROMPT_LIBRARY_PATH = f'{BASE_PATH}/data/prompt_library/clip_prompts_india_themes_semantic.json'\n",
        "\n",
        "VL_ENCODING_PATH = f'{BASE_PATH}/data/vl_encoding'\n",
        "EMBEDDINGS_PATH = f'{VL_ENCODING_PATH}/embeddings'\n",
        "PROMPTS_PATH = f'{VL_ENCODING_PATH}/prompts'\n",
        "REPORTS_PATH = f'{VL_ENCODING_PATH}/reports'\n",
        "\n",
        "os.makedirs(EMBEDDINGS_PATH, exist_ok=True)\n",
        "os.makedirs(f'{PROMPTS_PATH}/image_prompts', exist_ok=True)\n",
        "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Directories created\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 1: PROMPT VALIDATION\n",
        "\n",
        "================================================================\n",
        "\n",
        "    \n",
        "    Validates and analyzes text prompts for CLIP processing.\n",
        "    Features:\n",
        "    - Token length validation (CLIP limit: 77 tokens)    \n",
        "    - Quality metrics and reporting\n",
        "    "
      ],
      "metadata": {
        "id": "ivHg8o7RfPRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROMPT VALIDATION MODULE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class PromptValidator:\n",
        "    def __init__(self, processor: CLIPProcessor):\n",
        "        \"\"\"\n",
        "        Initialize validator with CLIP processor.\n",
        "\n",
        "        Args:\n",
        "            processor: CLIPProcessor for tokenization\n",
        "        \"\"\"\n",
        "        self.processor = processor\n",
        "        self.validation_log = []\n",
        "        print(\"PromptValidator initialized\")\n",
        "\n",
        "    def validate_prompt(self, prompt: str, prompt_id: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate a single prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt: Text prompt to validate\n",
        "            prompt_id: Optional identifier for logging\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with validation results\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "        warnings = []\n",
        "\n",
        "        if not prompt or not prompt.strip():\n",
        "            issues.append(\"Empty prompt\")\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'issues': issues,\n",
        "                'warnings': warnings,\n",
        "                'prompt': prompt,\n",
        "                'prompt_id': prompt_id\n",
        "            }\n",
        "\n",
        "        prompt_clean = prompt.strip()\n",
        "\n",
        "        char_length = len(prompt_clean)\n",
        "        if char_length < 5:\n",
        "            warnings.append(f\"Very short prompt ({char_length} chars)\")\n",
        "        if char_length > 150:\n",
        "            warnings.append(f\"Long prompt ({char_length} chars)\")\n",
        "\n",
        "        try:\n",
        "            tokens = self.processor.tokenizer(prompt_clean, truncation=False)\n",
        "            token_count = len(tokens['input_ids'])\n",
        "\n",
        "            if token_count > 77:\n",
        "                issues.append(f\"Exceeds CLIP token limit: {token_count}/77 tokens\")\n",
        "            elif token_count > 60:\n",
        "                warnings.append(f\"Near token limit: {token_count}/77 tokens\")\n",
        "\n",
        "        except Exception as e:\n",
        "            issues.append(f\"Tokenization failed: {str(e)}\")\n",
        "            token_count = -1\n",
        "\n",
        "        problem_chars = ['@', '#', '$', '%', '^', '*', '|', '\\\\']\n",
        "        found_chars = [c for c in problem_chars if c in prompt_clean]\n",
        "        if found_chars:\n",
        "            warnings.append(f\"Contains special characters: {found_chars}\")\n",
        "\n",
        "        result = {\n",
        "            'valid': len(issues) == 0,\n",
        "            'issues': issues,\n",
        "            'warnings': warnings,\n",
        "            'prompt': prompt_clean,\n",
        "            'prompt_id': prompt_id,\n",
        "            'char_length': char_length,\n",
        "            'token_count': token_count if token_count != -1 else None\n",
        "        }\n",
        "\n",
        "        self.validation_log.append(result)\n",
        "        return result\n",
        "\n",
        "    def validate_prompt_library(self, prompt_library: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate entire prompt library.\n",
        "\n",
        "        Args:\n",
        "            prompt_library: Dictionary of themes -> categories -> prompts\n",
        "\n",
        "        Returns:\n",
        "            Validation report with statistics\n",
        "        \"\"\"\n",
        "        print(\"Validating prompt library...\")\n",
        "\n",
        "        report = {\n",
        "            'total_prompts': 0,\n",
        "            'valid_prompts': 0,\n",
        "            'prompts_with_issues': 0,\n",
        "            'prompts_with_warnings': 0,\n",
        "            'issues_found': [],\n",
        "            'warnings_found': [],\n",
        "            'token_stats': {\n",
        "                'min': float('inf'),\n",
        "                'max': 0,\n",
        "                'mean': 0,\n",
        "                'over_limit': 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        token_counts = []\n",
        "\n",
        "        for theme, categories in prompt_library.items():\n",
        "            for category, prompts in categories.items():\n",
        "                for idx, prompt in enumerate(prompts):\n",
        "                    prompt_id = f\"{theme}/{category}/{idx}\"\n",
        "\n",
        "                    result = self.validate_prompt(prompt, prompt_id)\n",
        "                    report['total_prompts'] += 1\n",
        "\n",
        "                    if result['valid']:\n",
        "                        report['valid_prompts'] += 1\n",
        "                    else:\n",
        "                        report['prompts_with_issues'] += 1\n",
        "                        report['issues_found'].extend([\n",
        "                            {'prompt_id': prompt_id, 'issue': issue}\n",
        "                            for issue in result['issues']\n",
        "                        ])\n",
        "\n",
        "                    if result['warnings']:\n",
        "                        report['prompts_with_warnings'] += 1\n",
        "                        report['warnings_found'].extend([\n",
        "                            {'prompt_id': prompt_id, 'warning': warning}\n",
        "                            for warning in result['warnings']\n",
        "                        ])\n",
        "\n",
        "                    if result['token_count'] is not None:\n",
        "                        token_counts.append(result['token_count'])\n",
        "                        if result['token_count'] > 77:\n",
        "                            report['token_stats']['over_limit'] += 1\n",
        "\n",
        "        if token_counts:\n",
        "            report['token_stats']['min'] = int(min(token_counts))\n",
        "            report['token_stats']['max'] = int(max(token_counts))\n",
        "            report['token_stats']['mean'] = float(np.mean(token_counts))\n",
        "\n",
        "        return report\n",
        "\n",
        "    def generate_report(self, validation_results: Dict, output_file: str = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate human-readable validation report.\n",
        "\n",
        "        Args:\n",
        "            validation_results: Results from validate_prompt_library()\n",
        "            output_file: Optional path to save report\n",
        "\n",
        "        Returns:\n",
        "            Report text\n",
        "        \"\"\"\n",
        "        report_lines = []\n",
        "        report_lines.append(\"=\" * 80)\n",
        "        report_lines.append(\"PROMPT LIBRARY VALIDATION REPORT\")\n",
        "        report_lines.append(\"=\" * 80)\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        report_lines.append(\"SUMMARY\")\n",
        "        report_lines.append(\"-\" * 80)\n",
        "        report_lines.append(f\"Total prompts: {validation_results['total_prompts']}\")\n",
        "        report_lines.append(f\"Valid prompts: {validation_results['valid_prompts']}\")\n",
        "        report_lines.append(f\"Prompts with issues: {validation_results['prompts_with_issues']}\")\n",
        "        report_lines.append(f\"Prompts with warnings: {validation_results['prompts_with_warnings']}\")\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        report_lines.append(\"TOKEN STATISTICS\")\n",
        "        report_lines.append(\"-\" * 80)\n",
        "        stats = validation_results['token_stats']\n",
        "        report_lines.append(f\"Min tokens: {stats['min']}\")\n",
        "        report_lines.append(f\"Max tokens: {stats['max']}\")\n",
        "        report_lines.append(f\"Mean tokens: {stats['mean']:.1f}\")\n",
        "        report_lines.append(f\"Prompts over limit (77): {stats['over_limit']}\")\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        if validation_results['issues_found']:\n",
        "            report_lines.append(\"CRITICAL ISSUES\")\n",
        "            report_lines.append(\"-\" * 80)\n",
        "            for item in validation_results['issues_found']:\n",
        "                report_lines.append(f\"  [{item['prompt_id']}] {item['issue']}\")\n",
        "            report_lines.append(\"\")\n",
        "        else:\n",
        "            report_lines.append(\"No critical issues found\")\n",
        "            report_lines.append(\"\")\n",
        "\n",
        "        if validation_results['warnings_found']:\n",
        "            report_lines.append(\"WARNINGS\")\n",
        "            report_lines.append(\"-\" * 80)\n",
        "            for item in validation_results['warnings_found'][:10]:\n",
        "                report_lines.append(f\"  [{item['prompt_id']}] {item['warning']}\")\n",
        "            if len(validation_results['warnings_found']) > 10:\n",
        "                report_lines.append(f\"  ... and {len(validation_results['warnings_found']) - 10} more\")\n",
        "            report_lines.append(\"\")\n",
        "\n",
        "        report_lines.append(\"=\" * 80)\n",
        "\n",
        "        report_text = \"\\n\".join(report_lines)\n",
        "\n",
        "        if output_file:\n",
        "            with open(output_file, 'w') as f:\n",
        "                f.write(report_text)\n",
        "            print(f\"Report saved to: {output_file}\")\n",
        "\n",
        "        return report_text\n",
        "\n",
        "\n",
        "print(\"PromptValidator class ready\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyO9Qj2xfpSw",
        "outputId": "060455b8-55c6-44b7-d627-63f2e56fc315"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT VALIDATION MODULE\n",
            "================================================================================\n",
            "PromptValidator class ready\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 2: IMAGE PREPROCESSING\n",
        "\n",
        "================================================================\n",
        "\n",
        "    Comprehensive image preprocessing for CLIP model.\n",
        "    \n",
        "    Handles:\n",
        "    - EXIF orientation correction\n",
        "    - RGB conversion from any color mode\n",
        "    - Aspect ratio preservation\n",
        "    - High-quality resampling\n",
        "    - White padding for consistent dimensions\n"
      ],
      "metadata": {
        "id": "STEI0hXMW1UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IMAGE PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class ImagePreprocessor:\n",
        "    \"\"\"Comprehensive image preprocessing for CLIP.\"\"\"\n",
        "\n",
        "    def __init__(self, target_size: Tuple[int, int] = (224, 224)):\n",
        "        self.target_size = target_size\n",
        "        print(f\"ImagePreprocessor initialized (target: {target_size})\")\n",
        "\n",
        "    def preprocess_image(self, image_path: str) -> Image.Image:\n",
        "        \"\"\"Load and preprocess image.\"\"\"\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            img = self._fix_orientation(img)\n",
        "\n",
        "            if img.mode != 'RGB':\n",
        "                if img.mode == 'RGBA':\n",
        "                    background = Image.new('RGB', img.size, (255, 255, 255))\n",
        "                    background.paste(img, mask=img.split()[3])\n",
        "                    img = background\n",
        "                else:\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "            img = self._resize_with_padding(img, self.target_size)\n",
        "            return img\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to preprocess {image_path}: {str(e)}\")\n",
        "\n",
        "    def _fix_orientation(self, img: Image.Image) -> Image.Image:\n",
        "        \"\"\"Fix EXIF orientation.\"\"\"\n",
        "        try:\n",
        "            img = ImageOps.exif_transpose(img)\n",
        "        except:\n",
        "            pass\n",
        "        return img\n",
        "\n",
        "    def _resize_with_padding(self, img: Image.Image, target_size: Tuple[int, int]) -> Image.Image:\n",
        "        \"\"\"Resize with aspect ratio preservation.\"\"\"\n",
        "        img_ratio = img.width / img.height\n",
        "        target_ratio = target_size[0] / target_size[1]\n",
        "\n",
        "        if img_ratio > target_ratio:\n",
        "            new_width = target_size[0]\n",
        "            new_height = int(new_width / img_ratio)\n",
        "        else:\n",
        "            new_height = target_size[1]\n",
        "            new_width = int(new_height * img_ratio)\n",
        "\n",
        "        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "        canvas = Image.new('RGB', target_size, (255, 255, 255))\n",
        "        offset_x = (target_size[0] - new_width) // 2\n",
        "        offset_y = (target_size[1] - new_height) // 2\n",
        "        canvas.paste(img, (offset_x, offset_y))\n",
        "\n",
        "        return canvas\n",
        "\n",
        "    def validate_image(self, image_path: str, max_size_mb: float = 10.0) -> Dict[str, Any]:\n",
        "        \"\"\"Validate image.\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(image_path):\n",
        "                return {'valid': False, 'error': 'File not found'}\n",
        "\n",
        "            size_mb = os.path.getsize(image_path) / (1024 * 1024)\n",
        "            if size_mb > max_size_mb:\n",
        "                return {'valid': False, 'error': f'File too large: {size_mb:.2f}MB'}\n",
        "\n",
        "            img = Image.open(image_path)\n",
        "            img_format = img.format\n",
        "            dimensions = img.size\n",
        "            img.close()\n",
        "\n",
        "            return {'valid': True, 'size_mb': size_mb, 'format': img_format, 'dimensions': dimensions}\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'valid': False, 'error': str(e)}\n",
        "\n",
        "\n",
        "preprocessor = ImagePreprocessor(target_size=(224, 224))\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aes9NejXFwI",
        "outputId": "a1a35ee5-5419-4034-d4a4-baad31d0d24f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "IMAGE PREPROCESSING\n",
            "================================================================================\n",
            "ImagePreprocessor initialized (target: (224, 224))\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 3: LOAD CLIP MODEL\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "k_p2F2toMO7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING CLIP MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "print(f\"Model: {model_name}\")\n",
        "print(\"Loading...\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "sample_text = [\"test\"]\n",
        "inputs = processor(text=sample_text, return_tensors=\"pt\", padding=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "with torch.no_grad():\n",
        "    test_output = model.get_text_features(**inputs)\n",
        "    if torch.is_tensor(test_output):\n",
        "        embedding_dim = test_output.shape[-1]\n",
        "    else:\n",
        "        embedding_dim = 512\n",
        "\n",
        "print(f\"\\nModel loaded\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
        "print(f\"  Embedding dimension: {embedding_dim}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "a1351f75a5be488a8973ef791b958b89",
            "86f88d43b4224f3a84fe45bd97425679",
            "1379444c57864e379be8de023d2f960c",
            "6bafb8b5798d4bb2b4706f26bd25a4c4",
            "17105168b42d447ea0a30ed8e976cb2f",
            "91a2729f3999460f89365bd971d10e47",
            "4a26263b84d24c0da78ee3b90543c3d6",
            "99b2590c1a8d4a6a92677cda5bedef65",
            "37e3446252464dcb8faac6c0aa5d6256",
            "268f7d3acc574ad59e3a590030902e99",
            "c2b5898302c44b7281db0dcd4b4c9246"
          ]
        },
        "id": "rytAhN1EMePu",
        "outputId": "c7915418-d169-48f1-8cce-92b9bd4d0c19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING CLIP MODEL\n",
            "================================================================================\n",
            "Device: cpu\n",
            "Model: openai/clip-vit-base-patch32\n",
            "Loading...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1351f75a5be488a8973ef791b958b89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIPModel LOAD REPORT from: openai/clip-vit-base-patch32\n",
            "Key                                  | Status     |  | \n",
            "-------------------------------------+------------+--+-\n",
            "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
            "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "The image processor of type `CLIPImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model loaded\n",
            "  Parameters: 151.3M\n",
            "  Embedding dimension: 512\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 4: LOAD IMAGE META DATA  AND PROMPT LIBRARY\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "lqSPsvYAZxXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with open(METADATA_PATH, 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"Metadata: {metadata['total_images']} images, {metadata['total_destinations']} destinations\")\n",
        "\n",
        "# ADD THIS: Initialize pipeline_status if it doesn't exist\n",
        "if 'pipeline_status' not in metadata:\n",
        "    metadata['pipeline_status'] = {\n",
        "        'embeddings_computed': False,\n",
        "        'prompts_extracted': False,\n",
        "        'prompts_validated': False\n",
        "    }\n",
        "    print(\"Initialized pipeline_status in metadata\")\n",
        "\n",
        "with open(PROMPT_LIBRARY_PATH, 'r') as f:\n",
        "    prompt_library = json.load(f)\n",
        "\n",
        "total_prompts = sum(sum(len(prompts) for prompts in categories.values())\n",
        "                    for categories in prompt_library.values())\n",
        "print(f\"Prompt library: {total_prompts} prompts\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnCPCUKmhoKb",
        "outputId": "3acb0bd4-81ea-4af0-f3c4-d08013350b27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING DATA\n",
            "================================================================================\n",
            "Metadata: 215 images, 47 destinations\n",
            "Prompt library: 2200 prompts\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 5: VALIDATE PROMPT LIBRARY\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "U2wWkTmVZu3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDATING PROMPT LIBRARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "validator = PromptValidator(processor)\n",
        "validation_results = validator.validate_prompt_library(prompt_library)\n",
        "\n",
        "validation_report = validator.generate_report(\n",
        "    validation_results,\n",
        "    output_file=f'{REPORTS_PATH}/prompt_validation_report.txt'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + validation_report)\n",
        "\n",
        "if validation_results['prompts_with_issues'] > 0:\n",
        "    print(f\"\\nWARNING: {validation_results['prompts_with_issues']} prompts have critical issues!\")\n",
        "    print(\"Review the validation report before proceeding.\")\n",
        "else:\n",
        "    print(\"\\nAll prompts passed validation!\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT11xeQeh8WK",
        "outputId": "63f82c2a-8f5c-477a-dacf-3fedfd6c447d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "VALIDATING PROMPT LIBRARY\n",
            "================================================================================\n",
            "PromptValidator initialized\n",
            "Validating prompt library...\n",
            "Report saved to: /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/reports/prompt_validation_report.txt\n",
            "\n",
            "================================================================================\n",
            "PROMPT LIBRARY VALIDATION REPORT\n",
            "================================================================================\n",
            "\n",
            "SUMMARY\n",
            "--------------------------------------------------------------------------------\n",
            "Total prompts: 2200\n",
            "Valid prompts: 2200\n",
            "Prompts with issues: 0\n",
            "Prompts with warnings: 0\n",
            "\n",
            "TOKEN STATISTICS\n",
            "--------------------------------------------------------------------------------\n",
            "Min tokens: 17\n",
            "Max tokens: 28\n",
            "Mean tokens: 21.3\n",
            "Prompts over limit (77): 0\n",
            "\n",
            "No critical issues found\n",
            "\n",
            "================================================================================\n",
            "\n",
            "All prompts passed validation!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "EMBEDDING EXTRACTION FUNCTIONS\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "ZubUbVch8Tlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEFINING EXTRACTION FUNCTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def extract_clip_features(outputs):\n",
        "    \"\"\"Universal fix for extracting tensors from CLIP outputs.\"\"\"\n",
        "    # PRIORITY 1: Direct tensor\n",
        "    if torch.is_tensor(outputs):\n",
        "        return outputs\n",
        "\n",
        "    # PRIORITY 2: pooler_output (CLIP's projected features)\n",
        "    if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "        return outputs.pooler_output\n",
        "\n",
        "    # PRIORITY 3: text_embeds or image_embeds (newer CLIP versions)\n",
        "    if hasattr(outputs, 'text_embeds') and outputs.text_embeds is not None:\n",
        "        return outputs.text_embeds\n",
        "    if hasattr(outputs, 'image_embeds') and outputs.image_embeds is not None:\n",
        "        return outputs.image_embeds\n",
        "\n",
        "    # PRIORITY 4: last_hidden_state (take CLS token)\n",
        "    if hasattr(outputs, 'last_hidden_state') and outputs.last_hidden_state is not None:\n",
        "        return outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "    # PRIORITY 5: Try indexing\n",
        "    try:\n",
        "        return outputs[0]\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # PRIORITY 6: Try converting to tensor\n",
        "    try:\n",
        "        return torch.tensor(outputs)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    raise ValueError(f\"Could not extract 512-dim projected CLIP features from model output type: {type(outputs)}. Object: {outputs}\")\n",
        "\n",
        "\n",
        "\n",
        "def extract_image_embedding(image_path, model, processor, preprocessor, device):\n",
        "    \"\"\"Extract CLIP embedding from image with preprocessing.\"\"\"\n",
        "    try:\n",
        "        validation = preprocessor.validate_image(image_path)\n",
        "        if not validation['valid']:\n",
        "            print(f\"Skipping {image_path}: {validation['error']}\")\n",
        "            return None\n",
        "\n",
        "        img = preprocessor.preprocess_image(image_path)\n",
        "\n",
        "        inputs = processor(images=img, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            image_features = extract_clip_features(outputs)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return image_features.cpu().numpy()[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR in extract_image_embedding for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_embeddings_batch(texts, model, processor, device):\n",
        "    \"\"\"Extract CLIP embeddings for batch of text prompts.\"\"\"\n",
        "    try:\n",
        "        inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            text_features = extract_clip_features(outputs)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return text_features.cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR in extract_text_embeddings_batch: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"Extraction functions defined\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzeBALAs8TX_",
        "outputId": "468361f6-f280-4dcb-ffad-619ece684877"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DEFINING EXTRACTION FUNCTIONS\n",
            "================================================================================\n",
            "Extraction functions defined\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 6: PRE-COMPUTE PROMPT EMBEDDINGS\n",
        "\n",
        "\n",
        "    Pre-compute embeddings for all 2,200 prompts\n",
        "    \n",
        "    FIXED: Properly extract tensor from CLIP output object\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with structure:\n",
        "        {\n",
        "            'Beach': {\n",
        "                'LandscapeType': [\n",
        "                    {'text': 'prompt text', 'embedding': np.array(512,)},\n",
        "                    ...\n",
        "                ],\n",
        "                ...\n",
        "            },\n",
        "            ...\n",
        "        }\n",
        "\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "o2F2rsjzNn8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 6: PRE-COMPUTING PROMPT EMBEDDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def precompute_prompt_embeddings(prompt_library, model, processor, device):\n",
        "    \"\"\"Pre-compute embeddings for all prompts.\"\"\"\n",
        "\n",
        "    prompt_embeddings = {}\n",
        "    total_prompts = sum(sum(len(p) for p in cats.values()) for cats in prompt_library.values())\n",
        "\n",
        "    print(f\"Total prompts: {total_prompts}\")\n",
        "\n",
        "    with tqdm(total=total_prompts, desc=\"Computing prompt embeddings\") as pbar:\n",
        "        for theme, categories in prompt_library.items():\n",
        "            prompt_embeddings[theme] = {}\n",
        "\n",
        "            for category, prompts in categories.items():\n",
        "                prompt_embeddings[theme][category] = []\n",
        "\n",
        "                batch_size = 32\n",
        "                for i in range(0, len(prompts), batch_size):\n",
        "                    batch = prompts[i:i+batch_size]\n",
        "\n",
        "                    embeddings = extract_text_embeddings_batch(batch, model, processor, device)\n",
        "\n",
        "                    if embeddings is not None:\n",
        "                        for j, prompt_text in enumerate(batch):\n",
        "                            prompt_embeddings[theme][category].append({\n",
        "                                'text': prompt_text,\n",
        "                                'embedding': embeddings[j]\n",
        "                            })\n",
        "\n",
        "                    pbar.update(len(batch))\n",
        "\n",
        "    return prompt_embeddings\n",
        "\n",
        "\n",
        "prompt_embeddings_file = f'{EMBEDDINGS_PATH}/prompt_embeddings.pkl'\n",
        "\n",
        "# Force re-computation to ensure consistency after fix\n",
        "force_recompute_prompts = True\n",
        "\n",
        "if os.path.exists(prompt_embeddings_file) and not force_recompute_prompts:\n",
        "    print(\"Found cached prompt embeddings...\")\n",
        "\n",
        "    with open(prompt_embeddings_file, 'rb') as f:\n",
        "        cached_embeddings = pickle.load(f)\n",
        "\n",
        "    for theme, cats in cached_embeddings.items():\n",
        "        for cat, prompts in cats.items():\n",
        "            if prompts:\n",
        "                cached_dim = prompts[0]['embedding'].shape[0]\n",
        "                break\n",
        "        break\n",
        "\n",
        "    print(f\"  Cached dimension: {cached_dim}\")\n",
        "    print(f\"  Current model dimension: {embedding_dim}\")\n",
        "\n",
        "    if cached_dim == embedding_dim:\n",
        "        print(\"  Dimensions match - using cache\")\n",
        "        prompt_embeddings = cached_embeddings\n",
        "    else:\n",
        "        print(\"  Dimension mismatch - recomputing...\")\n",
        "        os.remove(prompt_embeddings_file)\n",
        "        prompt_embeddings = precompute_prompt_embeddings(prompt_library, model, processor, device)\n",
        "\n",
        "        with open(prompt_embeddings_file, 'wb') as f:\n",
        "            pickle.dump(prompt_embeddings, f)\n",
        "        print(f\"  Saved new embeddings\")\n",
        "else:\n",
        "    if os.path.exists(prompt_embeddings_file) and force_recompute_prompts:\n",
        "        print(\"Forcing recomputation of prompt embeddings, ignoring cached file.\")\n",
        "        os.remove(prompt_embeddings_file) # Ensure old cache is removed\n",
        "    else:\n",
        "        print(\"Computing prompt embeddings...\")\n",
        "\n",
        "    prompt_embeddings = precompute_prompt_embeddings(prompt_library, model, processor, device)\n",
        "\n",
        "    with open(prompt_embeddings_file, 'wb') as f:\n",
        "        pickle.dump(prompt_embeddings, f)\n",
        "    print(f\"Saved to: {prompt_embeddings_file}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwgsq_1C7zl-",
        "outputId": "515aa661-7d13-439b-d585-c32515a2ec36"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 6: PRE-COMPUTING PROMPT EMBEDDINGS\n",
            "================================================================================\n",
            "Forcing recomputation of prompt embeddings, ignoring cached file.\n",
            "Total prompts: 2200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing prompt embeddings: 100%|██████████| 2200/2200 [01:33<00:00, 23.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to: /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/prompt_embeddings.pkl\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 7 IMAGE EMBEDDING EXTRACTION (WITH PREPROCESSING)\n",
        "\n",
        "\n",
        "    Extract CLIP embedding with comprehensive preprocessing.\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Validate image\n",
        "    2. Preprocess (EXIF, RGB conversion, aspect ratio)\n",
        "    3. CLIP processor (normalization, tensor conversion)\n",
        "    4. Extract embedding\n",
        "    5. L2 normalization\n",
        "    \n",
        "    FIXED: Properly handles CLIP output tensors\n",
        "\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "EM2MIyxRRfcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_prompts_for_image(image_embedding, theme, prompt_embeddings, top_k=2):\n",
        "    \"\"\"Extract top prompts using theme-first approach.\"\"\"\n",
        "\n",
        "    if theme not in prompt_embeddings:\n",
        "        return {}\n",
        "\n",
        "    theme_prompts = prompt_embeddings[theme]\n",
        "    extracted_prompts = {}\n",
        "\n",
        "    for category, prompts in theme_prompts.items():\n",
        "        scores = []\n",
        "\n",
        "        for prompt_data in prompts:\n",
        "            score = np.dot(image_embedding, prompt_data['embedding'])\n",
        "            scores.append((prompt_data['text'], float(score)))\n",
        "\n",
        "        top_prompts = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        #filtered = [(text, score) for text, score in top_prompts if score > 0.40]\n",
        "        filtered = [(text, score) for text, score in top_prompts if score > 0.25]  # Changed from 0.40\n",
        "\n",
        "        if filtered:\n",
        "            extracted_prompts[category] = [\n",
        "                {'text': text, 'score': score} for text, score in filtered\n",
        "            ]\n",
        "\n",
        "    return extracted_prompts\n",
        "\n",
        "\n",
        "\n",
        "def process_all_images(metadata, landmarks_path, model, processor, preprocessor, device):\n",
        "    \"\"\"Process all images with preprocessing.\"\"\"\n",
        "\n",
        "    image_embeddings = {}\n",
        "    image_metadata_list = {}\n",
        "\n",
        "    total_images = metadata['total_images']\n",
        "    print(f\"Processing {total_images} images...\")\n",
        "\n",
        "    stats = {'processed': 0, 'failed': 0}\n",
        "\n",
        "    with tqdm(total=total_images, desc=\"Extracting embeddings\") as pbar:\n",
        "        for theme in metadata['themes']:\n",
        "            theme_name = theme['theme_name']\n",
        "\n",
        "            for state in theme['states']:\n",
        "                state_name = state['state_name']\n",
        "\n",
        "                for destination in state['destinations']:\n",
        "                    dest_id = destination['destination_id']\n",
        "                    dest_folder = destination['folder']\n",
        "\n",
        "                    for img_filename in destination['images']:\n",
        "                        image_id = f\"{dest_id}_{img_filename.replace('.jpg', '').replace('.png', '')}\"\n",
        "                        image_path = os.path.join(landmarks_path, dest_folder, img_filename)\n",
        "\n",
        "                        embedding = extract_image_embedding(image_path, model, processor, preprocessor, device)\n",
        "\n",
        "                        if embedding is not None:\n",
        "                            image_embeddings[image_id] = embedding\n",
        "\n",
        "                            image_metadata_list[image_id] = {\n",
        "                                'image_path': image_path,\n",
        "                                'filename': img_filename,\n",
        "                                'destination_id': dest_id,\n",
        "                                'destination_name': destination['destination_name'],\n",
        "                                'theme': theme_name,\n",
        "                                'state': state_name,\n",
        "                                'folder': dest_folder\n",
        "                            }\n",
        "                            stats['processed'] += 1\n",
        "                        else:\n",
        "                            stats['failed'] += 1\n",
        "\n",
        "                        pbar.update(1)\n",
        "\n",
        "    print(f\"\\nProcessed: {stats['processed']}\")\n",
        "    if stats['failed'] > 0:\n",
        "        print(f\"Failed: {stats['failed']}\")\n",
        "\n",
        "    return image_embeddings, image_metadata_list, stats\n",
        "\n",
        "\n",
        "image_embeddings, image_metadata_dict, validation_stats = process_all_images(\n",
        "    metadata, LANDMARKS_PATH, model, processor, preprocessor, device\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKJmQWNfUwKQ",
        "outputId": "fdf81257-fd38-4605-c224-18ed3995a1aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 215 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting embeddings: 100%|██████████| 215/215 [00:55<00:00,  3.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed: 215\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===========================================================================================================\n",
        "\n",
        "STEP 8: Prompt Extraction For all images\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "IugOCdtmpw7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_prompts(image_embeddings, image_metadata_dict, prompt_embeddings):\n",
        "    \"\"\"Extract prompts for all images.\"\"\"\n",
        "\n",
        "    all_image_prompts = {}\n",
        "\n",
        "    print(f\"Extracting prompts for {len(image_embeddings)} images...\")\n",
        "\n",
        "    with tqdm(total=len(image_embeddings), desc=\"Extracting prompts\") as pbar:\n",
        "        for image_id, embedding in image_embeddings.items():\n",
        "            metadata = image_metadata_dict[image_id]\n",
        "            theme = metadata['theme']\n",
        "\n",
        "            prompts = extract_prompts_for_image(embedding, theme, prompt_embeddings, top_k=2)\n",
        "\n",
        "            all_image_prompts[image_id] = {\n",
        "                'image_id': image_id,\n",
        "                'theme': theme,\n",
        "                'destination_id': metadata['destination_id'],\n",
        "                'extracted_prompts': prompts,\n",
        "                'total_prompts_extracted': sum(len(p) for p in prompts.values())\n",
        "            }\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(f\"Extracted prompts for {len(all_image_prompts)} images\")\n",
        "    return all_image_prompts\n",
        "\n",
        "\n",
        "all_image_prompts = process_all_prompts(image_embeddings, image_metadata_dict, prompt_embeddings)\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhLHvhz7p-7E",
        "outputId": "0619587b-e540-4d31-98c5-3430ae381a1d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting prompts for 215 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting prompts: 100%|██████████| 215/215 [00:00<00:00, 1212.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted prompts for 215 images\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===========================================================================================================\n",
        "\n",
        "STEP 9: AGGREGATE PER DESTINATION\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "CoLM5fnIoZFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 9: AGGREGATING BY DESTINATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def aggregate_destination_embeddings(image_embeddings, image_metadata_dict):\n",
        "    \"\"\"Aggregate embeddings per destination.\"\"\"\n",
        "\n",
        "    destination_embeddings = {}\n",
        "    destination_image_embeddings = {}\n",
        "\n",
        "    for image_id, embedding in image_embeddings.items():\n",
        "        dest_id = image_metadata_dict[image_id]['destination_id']\n",
        "\n",
        "        if dest_id not in destination_image_embeddings:\n",
        "            destination_image_embeddings[dest_id] = []\n",
        "\n",
        "        destination_image_embeddings[dest_id].append(embedding)\n",
        "\n",
        "    for dest_id, embeddings in destination_image_embeddings.items():\n",
        "        embeddings_array = np.array(embeddings)\n",
        "        avg_embedding = np.mean(embeddings_array, axis=0)\n",
        "        avg_embedding = avg_embedding / np.linalg.norm(avg_embedding)\n",
        "\n",
        "        destination_embeddings[dest_id] = {\n",
        "            'average_embedding': avg_embedding,\n",
        "            'individual_embeddings': embeddings_array,\n",
        "            'num_images': len(embeddings)\n",
        "        }\n",
        "\n",
        "    print(f\"Aggregated {len(destination_embeddings)} destinations\")\n",
        "    return destination_embeddings\n",
        "\n",
        "\n",
        "def aggregate_destination_prompts(all_image_prompts):\n",
        "    \"\"\"Aggregate prompts with weighted scoring.\"\"\"\n",
        "\n",
        "    destination_prompts = {}\n",
        "    dest_groups = {}\n",
        "\n",
        "    for image_id, data in all_image_prompts.items():\n",
        "        dest_id = data['destination_id']\n",
        "        if dest_id not in dest_groups:\n",
        "            dest_groups[dest_id] = []\n",
        "        dest_groups[dest_id].append(data)\n",
        "\n",
        "    for dest_id, images_data in dest_groups.items():\n",
        "        num_images = len(images_data)\n",
        "        category_prompts = {}\n",
        "\n",
        "        for img_data in images_data:\n",
        "            for category, prompts in img_data['extracted_prompts'].items():\n",
        "                if category not in category_prompts:\n",
        "                    category_prompts[category] = []\n",
        "                for prompt in prompts:\n",
        "                    category_prompts[category].append(prompt)\n",
        "\n",
        "        aggregated = {}\n",
        "\n",
        "        for category, prompts in category_prompts.items():\n",
        "            prompt_stats = {}\n",
        "\n",
        "            for prompt in prompts:\n",
        "                text = prompt['text']\n",
        "                score = prompt['score']\n",
        "\n",
        "                if text not in prompt_stats:\n",
        "                    prompt_stats[text] = {'scores': [], 'count': 0}\n",
        "\n",
        "                prompt_stats[text]['scores'].append(score)\n",
        "                prompt_stats[text]['count'] += 1\n",
        "\n",
        "            weighted_prompts = []\n",
        "            for text, stats in prompt_stats.items():\n",
        "                avg_score = np.mean(stats['scores'])\n",
        "                frequency = stats['count'] / num_images\n",
        "                weighted_score = (avg_score * 0.6) + (frequency * 0.4)\n",
        "\n",
        "                weighted_prompts.append({\n",
        "                    'text': text,\n",
        "                    'avg_score': float(avg_score),\n",
        "                    'frequency': float(frequency),\n",
        "                    'weighted_score': float(weighted_score),\n",
        "                    'appearances': stats['count']\n",
        "                })\n",
        "\n",
        "            top_prompts = sorted(weighted_prompts, key=lambda x: x['weighted_score'], reverse=True)[:2]\n",
        "\n",
        "            if top_prompts:\n",
        "                aggregated[category] = top_prompts\n",
        "\n",
        "        destination_prompts[dest_id] = {\n",
        "            'destination_id': dest_id,\n",
        "            'num_images': num_images,\n",
        "            'aggregated_prompts': aggregated,\n",
        "            'dominant_characteristics': {\n",
        "                cat: prompts[0]['text'] for cat, prompts in aggregated.items() if prompts\n",
        "            }\n",
        "        }\n",
        "\n",
        "    print(f\"Aggregated prompts for {len(destination_prompts)} destinations\")\n",
        "    return destination_prompts\n",
        "\n",
        "\n",
        "destination_embeddings = aggregate_destination_embeddings(image_embeddings, image_metadata_dict)\n",
        "destination_prompts = aggregate_destination_prompts(all_image_prompts)\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZHRKHfMouhz",
        "outputId": "d48b468d-7d71-41eb-ec8a-604ddc4df7f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 9: AGGREGATING BY DESTINATION\n",
            "================================================================================\n",
            "Aggregated 47 destinations\n",
            "Aggregated prompts for 47 destinations\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==========================================================================================================\n",
        "STEP 10: SAVE ALL DATA\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "cPaO9oCosokD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 10: SAVING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Saving embeddings...\")\n",
        "\n",
        "image_ids = list(image_embeddings.keys())\n",
        "embeddings_array = np.array([image_embeddings[img_id] for img_id in image_ids])\n",
        "\n",
        "destination_ids = list(destination_embeddings.keys())\n",
        "dest_avg_embs = np.array([destination_embeddings[d_id]['average_embedding'] for d_id in destination_ids])\n",
        "\n",
        "np.savez(\n",
        "    f'{EMBEDDINGS_PATH}/all_embeddings.npz',\n",
        "    image_ids=image_ids,\n",
        "    image_embeddings=embeddings_array,\n",
        "    destination_ids=destination_ids,\n",
        "    destination_embeddings=dest_avg_embs\n",
        ")\n",
        "\n",
        "print(f\"all_embeddings.npz ({embeddings_array.shape})\")\n",
        "\n",
        "embedding_index = {\n",
        "    'image_index': {img_id: idx for idx, img_id in enumerate(image_ids)},\n",
        "    'destination_index': {d_id: idx for idx, d_id in enumerate(destination_ids)},\n",
        "    'metadata': {\n",
        "        'created_date': datetime.now().isoformat(),\n",
        "        'total_images': len(image_ids),\n",
        "        'total_destinations': len(destination_ids),\n",
        "        'embedding_dim': embeddings_array.shape[1],\n",
        "        'model_name': model_name,\n",
        "        'preprocessing_enabled': True,\n",
        "        'validation_stats': validation_stats,\n",
        "        'prompt_validation': validation_results\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{EMBEDDINGS_PATH}/embedding_index.json', 'w') as f:\n",
        "    json.dump(embedding_index, f, indent=2)\n",
        "\n",
        "print(f\"embedding_index.json\")\n",
        "\n",
        "with open(f'{PROMPTS_PATH}/image_prompts.json', 'w') as f:\n",
        "    json.dump(all_image_prompts, f, indent=2)\n",
        "\n",
        "print(f\"image_prompts.json\")\n",
        "\n",
        "with open(f'{PROMPTS_PATH}/destination_prompts.json', 'w') as f:\n",
        "    json.dump(destination_prompts, f, indent=2)\n",
        "\n",
        "print(f\"destination_prompts.json\")\n",
        "\n",
        "with open(f'{EMBEDDINGS_PATH}/destination_embeddings_detailed.pkl', 'wb') as f:\n",
        "    pickle.dump(destination_embeddings, f)\n",
        "\n",
        "print(f\"destination_embeddings_detailed.pkl\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr_ni667soSl",
        "outputId": "f34d33a5-cb27-4140-853c-0f3d2c61fcdf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 10: SAVING DATA\n",
            "================================================================================\n",
            "Saving embeddings...\n",
            "all_embeddings.npz ((215, 512))\n",
            "embedding_index.json\n",
            "image_prompts.json\n",
            "destination_prompts.json\n",
            "destination_embeddings_detailed.pkl\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==========================================================================================================\n",
        "STEP 11: UPDATE METADATA\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "1a9BAqNStAl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 11: UPDATING METADATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "metadata['pipeline_status']['embeddings_computed'] = True\n",
        "metadata['pipeline_status']['prompts_extracted'] = True\n",
        "metadata['pipeline_status']['prompts_validated'] = True\n",
        "metadata['last_updated'] = datetime.now().isoformat()\n",
        "metadata['vl_encoding_version'] = '4.1'\n",
        "\n",
        "for theme in metadata['themes']:\n",
        "    for state in theme['states']:\n",
        "        for destination in state['destinations']:\n",
        "            dest_id = destination['destination_id']\n",
        "\n",
        "            if dest_id in destination_embeddings:\n",
        "                destination['embeddings_computed'] = True\n",
        "                destination['prompts_extracted'] = True\n",
        "\n",
        "                destination['embedding_references'] = {\n",
        "                    'embeddings_file': 'vl_encoding/embeddings/all_embeddings.npz',\n",
        "                    'destination_index': embedding_index['destination_index'][dest_id]\n",
        "                }\n",
        "\n",
        "                if dest_id in destination_prompts:\n",
        "                    destination['dominant_prompts'] = destination_prompts[dest_id]['dominant_characteristics']\n",
        "\n",
        "with open(METADATA_PATH, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"Updated metadata.json\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dymhKVpitAXs",
        "outputId": "f89a04d6-54c6-421c-e142-a3fcf0eab764"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 11: UPDATING METADATA\n",
            "================================================================================\n",
            "Updated metadata.json\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==========================================================================================================\n",
        "\n",
        "**STEP 12: VL Encoding and Verification Check**\n",
        "\n",
        "Verify prompt extraction quality and alignment statistics\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "UKu8c9okUA4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"VL ENCODING VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Paths\n",
        "BASE_PATH = '/content/drive/MyDrive/visual-intelligence-travel-finance'\n",
        "IMAGE_PROMPTS_PATH = f'{BASE_PATH}/data/vl_encoding/prompts/image_prompts.json'\n",
        "DEST_PROMPTS_PATH = f'{BASE_PATH}/data/vl_encoding/prompts/destination_prompts.json'\n",
        "METADATA_PATH = f'{BASE_PATH}/data/landmarks/metadata.json'\n",
        "\n",
        "# Load data\n",
        "print(\"\\nLoading data...\")\n",
        "with open(IMAGE_PROMPTS_PATH, 'r') as f:\n",
        "    image_prompts = json.load(f)\n",
        "\n",
        "with open(DEST_PROMPTS_PATH, 'r') as f:\n",
        "    destination_prompts = json.load(f)\n",
        "\n",
        "with open(METADATA_PATH, 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"Image prompts: {len(image_prompts)} images\")\n",
        "print(f\"Destination prompts: {len(destination_prompts)} destinations\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnlGfmtjUAqD",
        "outputId": "b526c32e-d449-49c2-ff2e-2ccf9732efbc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "VL ENCODING VERIFICATION\n",
            "================================================================================\n",
            "\n",
            "Loading data...\n",
            "Image prompts: 215 images\n",
            "Destination prompts: 47 destinations\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 1: OVERALL STATISTICS\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"1. OVERALL STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "total_images = len(image_prompts)\n",
        "total_prompts_extracted = sum(data['total_prompts_extracted'] for data in image_prompts.values())\n",
        "avg_prompts_per_image = total_prompts_extracted / total_images if total_images > 0 else 0\n",
        "\n",
        "print(f\"Total images processed: {total_images}\")\n",
        "print(f\"Total prompts extracted: {total_prompts_extracted}\")\n",
        "print(f\"Average prompts per image: {avg_prompts_per_image:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IrsjhXzdsM2",
        "outputId": "812d2612-88bd-4a37-bc0e-f6b87c53dcb4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "1. OVERALL STATISTICS\n",
            "================================================================================\n",
            "Total images processed: 215\n",
            "Total prompts extracted: 4403\n",
            "Average prompts per image: 20.48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 2: CATEGORY COVERAGE\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"2. CATEGORY COVERAGE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_categories = set()\n",
        "category_counts = Counter()\n",
        "images_per_category = defaultdict(int)\n",
        "\n",
        "for image_id, data in image_prompts.items():\n",
        "    for category in data['extracted_prompts'].keys():\n",
        "        all_categories.add(category)\n",
        "        category_counts[category] += len(data['extracted_prompts'][category])\n",
        "        images_per_category[category] += 1\n",
        "\n",
        "print(f\"Total unique categories: {len(all_categories)}\")\n",
        "print(f\"\\nCategories found: {', '.join(sorted(all_categories))}\")\n",
        "\n",
        "print(\"\\nPrompts per category:\")\n",
        "for category, count in category_counts.most_common():\n",
        "    coverage = (images_per_category[category] / total_images) * 100\n",
        "    print(f\"  {category:20s}: {count:4d} prompts ({images_per_category[category]:3d} images, {coverage:5.1f}% coverage)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRja8TmgfNOd",
        "outputId": "1390b8c1-6e04-430e-c4e0-911076e4f4e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "2. CATEGORY COVERAGE\n",
            "================================================================================\n",
            "Total unique categories: 11\n",
            "\n",
            "Categories found: Accessibility, Activities, Atmosphere, CrowdDensity, EconomyBudget, LandscapeType, NaturalVsCultural, RegionalStyle, VegetationType, VisualQuality, WaterFeatures\n",
            "\n",
            "Prompts per category:\n",
            "  WaterFeatures       :  412 prompts (206 images,  95.8% coverage)\n",
            "  RegionalStyle       :  408 prompts (204 images,  94.9% coverage)\n",
            "  Atmosphere          :  408 prompts (205 images,  95.3% coverage)\n",
            "  NaturalVsCultural   :  406 prompts (204 images,  94.9% coverage)\n",
            "  Activities          :  406 prompts (205 images,  95.3% coverage)\n",
            "  LandscapeType       :  405 prompts (205 images,  95.3% coverage)\n",
            "  CrowdDensity        :  402 prompts (203 images,  94.4% coverage)\n",
            "  Accessibility       :  399 prompts (201 images,  93.5% coverage)\n",
            "  VisualQuality       :  392 prompts (197 images,  91.6% coverage)\n",
            "  EconomyBudget       :  392 prompts (198 images,  92.1% coverage)\n",
            "  VegetationType      :  373 prompts (191 images,  88.8% coverage)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 3: PROMPTS PER IMAGE DISTRIBUTION\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"3. PROMPTS PER IMAGE DISTRIBUTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "prompts_distribution = Counter()\n",
        "for data in image_prompts.values():\n",
        "    prompts_distribution[data['total_prompts_extracted']] += 1\n",
        "\n",
        "print(\"Distribution of prompts extracted per image:\")\n",
        "for num_prompts in sorted(prompts_distribution.keys()):\n",
        "    count = prompts_distribution[num_prompts]\n",
        "    percentage = (count / total_images) * 100\n",
        "    bar = \"█\" * int(percentage / 2)\n",
        "    print(f\"  {num_prompts:2d} prompts: {count:4d} images ({percentage:5.1f}%) {bar}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oI8BQ1cfsjm",
        "outputId": "d242d217-d9d7-42c3-e8d7-edd7f9003f7e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "3. PROMPTS PER IMAGE DISTRIBUTION\n",
            "================================================================================\n",
            "Distribution of prompts extracted per image:\n",
            "   0 prompts:    2 images (  0.9%) \n",
            "   1 prompts:    2 images (  0.9%) \n",
            "   2 prompts:    2 images (  0.9%) \n",
            "   7 prompts:    2 images (  0.9%) \n",
            "   8 prompts:    1 images (  0.5%) \n",
            "   9 prompts:    1 images (  0.5%) \n",
            "  10 prompts:    1 images (  0.5%) \n",
            "  11 prompts:    2 images (  0.9%) \n",
            "  13 prompts:    1 images (  0.5%) \n",
            "  15 prompts:    1 images (  0.5%) \n",
            "  16 prompts:    3 images (  1.4%) \n",
            "  18 prompts:    3 images (  1.4%) \n",
            "  19 prompts:    2 images (  0.9%) \n",
            "  20 prompts:   26 images ( 12.1%) ██████\n",
            "  21 prompts:    6 images (  2.8%) █\n",
            "  22 prompts:  160 images ( 74.4%) █████████████████████████████████████\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find images with no prompts\n",
        "no_prompts = [img_id for img_id, data in image_prompts.items() if data['total_prompts_extracted'] == 0]\n",
        "if no_prompts:\n",
        "    print(f\"\\nWARNING: {len(no_prompts)} images have NO prompts extracted!\")\n",
        "    print(\"Sample images with no prompts:\")\n",
        "    for img_id in no_prompts[:5]:\n",
        "        print(f\"  - {img_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dosl8oGdf6H8",
        "outputId": "b569adae-830c-4ec2-f911-725cb6c2370d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: 2 images have NO prompts extracted!\n",
            "Sample images with no prompts:\n",
            "  - WATERFALL_MEGHALAYA_SEVEN_SISTERS_FALLS_seven_sisters_falls_003\n",
            "  - WATERFALL_MEGHALAYA_SEVEN_SISTERS_FALLS_seven_sisters_falls_005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 4: SCORE DISTRIBUTION\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"4. SCORE DISTRIBUTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_scores = []\n",
        "for data in image_prompts.values():\n",
        "    for category, prompts in data['extracted_prompts'].items():\n",
        "        for prompt in prompts:\n",
        "            all_scores.append(prompt['score'])\n",
        "\n",
        "if all_scores:\n",
        "    print(f\"Total prompt-image matches: {len(all_scores)}\")\n",
        "    print(f\"Score statistics:\")\n",
        "    print(f\"  Min:    {min(all_scores):.4f}\")\n",
        "    print(f\"  Max:    {max(all_scores):.4f}\")\n",
        "    print(f\"  Mean:   {np.mean(all_scores):.4f}\")\n",
        "    print(f\"  Median: {np.median(all_scores):.4f}\")\n",
        "    print(f\"  Std:    {np.std(all_scores):.4f}\")\n",
        "\n",
        "    # Score ranges\n",
        "    print(\"\\nScore ranges:\")\n",
        "    ranges = [\n",
        "        (0.25, 0.30, 'Weak Match'),\n",
        "        (0.30, 0.35, 'Decent Match'),\n",
        "        (0.35, 1.00, 'Excellent match')\n",
        "    ]\n",
        "\n",
        "    for min_score, max_score, label in ranges:\n",
        "        count = sum(1 for s in all_scores if min_score <= s < max_score)\n",
        "        percentage = (count / len(all_scores)) * 100\n",
        "        print(f\"  {label:15s} ({min_score:.2f}-{max_score:.2f}): {count:4d} ({percentage:5.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuzfuMXrgHSc",
        "outputId": "6efa43c4-35dc-4553-ee1a-4beb11216aea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "4. SCORE DISTRIBUTION\n",
            "================================================================================\n",
            "Total prompt-image matches: 4403\n",
            "Score statistics:\n",
            "  Min:    0.2500\n",
            "  Max:    0.3571\n",
            "  Mean:   0.2959\n",
            "  Median: 0.2959\n",
            "  Std:    0.0221\n",
            "\n",
            "Score ranges:\n",
            "  Weak Match      (0.25-0.30): 2497 ( 56.7%)\n",
            "  Decent Match    (0.30-0.35): 1877 ( 42.6%)\n",
            "  Excellent match (0.35-1.00):   29 (  0.7%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 5: THEME-SPECIFIC ANALYSIS\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"5. THEME-SPECIFIC ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "theme_stats = defaultdict(lambda: {'images': 0, 'prompts': 0, 'categories': set()})\n",
        "\n",
        "for image_id, data in image_prompts.items():\n",
        "    theme = data['theme']\n",
        "    theme_stats[theme]['images'] += 1\n",
        "    theme_stats[theme]['prompts'] += data['total_prompts_extracted']\n",
        "    theme_stats[theme]['categories'].update(data['extracted_prompts'].keys())\n",
        "\n",
        "print(\"Statistics by theme:\")\n",
        "for theme, stats in sorted(theme_stats.items()):\n",
        "    avg_prompts = stats['prompts'] / stats['images'] if stats['images'] > 0 else 0\n",
        "    print(f\"\\n{theme}:\")\n",
        "    print(f\"  Images: {stats['images']}\")\n",
        "    print(f\"  Total prompts: {stats['prompts']}\")\n",
        "    print(f\"  Avg prompts/image: {avg_prompts:.2f}\")\n",
        "    print(f\"  Categories used: {len(stats['categories'])}\")\n",
        "    print(f\"  Categories: {', '.join(sorted(stats['categories']))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZNmEmz7h3Ud",
        "outputId": "3ef0312f-c64f-4fea-ee55-ba4fc87ce1cd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "5. THEME-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "Statistics by theme:\n",
            "\n",
            "Beach:\n",
            "  Images: 130\n",
            "  Total prompts: 2725\n",
            "  Avg prompts/image: 20.96\n",
            "  Categories used: 11\n",
            "  Categories: Accessibility, Activities, Atmosphere, CrowdDensity, EconomyBudget, LandscapeType, NaturalVsCultural, RegionalStyle, VegetationType, VisualQuality, WaterFeatures\n",
            "\n",
            "Waterfall:\n",
            "  Images: 85\n",
            "  Total prompts: 1678\n",
            "  Avg prompts/image: 19.74\n",
            "  Categories used: 11\n",
            "  Categories: Accessibility, Activities, Atmosphere, CrowdDensity, EconomyBudget, LandscapeType, NaturalVsCultural, RegionalStyle, VegetationType, VisualQuality, WaterFeatures\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 6: DESTINATION AGGREGATION QUALITY\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"6. DESTINATION AGGREGATION QUALITY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "dest_with_prompts = sum(1 for d in destination_prompts.values()\n",
        "                        if len(d.get('aggregated_prompts', {})) > 0)\n",
        "total_destinations = len(destination_prompts)\n",
        "\n",
        "print(f\"Destinations with prompts: {dest_with_prompts}/{total_destinations}\")\n",
        "\n",
        "# Sample destination analysis\n",
        "sample_dest_id = list(destination_prompts.keys())[0]\n",
        "sample = destination_prompts[sample_dest_id]\n",
        "\n",
        "print(f\"\\nSample destination: {sample_dest_id}\")\n",
        "print(f\"Number of images: {sample['num_images']}\")\n",
        "print(f\"Categories with prompts: {len(sample['aggregated_prompts'])}\")\n",
        "\n",
        "if sample['aggregated_prompts']:\n",
        "    print(\"\\nTop prompts per category:\")\n",
        "    for category, prompts in sorted(sample['aggregated_prompts'].items()):\n",
        "        print(f\"\\n  {category}:\")\n",
        "        for prompt in prompts[:2]:\n",
        "            print(f\"    - {prompt['text']}\")\n",
        "            print(f\"      (avg: {prompt['avg_score']:.3f}, freq: {prompt['frequency']:.2f}, weighted: {prompt['weighted_score']:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1aPM215iFsS",
        "outputId": "b0734e57-406d-4d9c-b4b4-0cc591edbcf3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "6. DESTINATION AGGREGATION QUALITY\n",
            "================================================================================\n",
            "Destinations with prompts: 47/47\n",
            "\n",
            "Sample destination: BEACH_GOA_AGONDA_BEACH\n",
            "Number of images: 1\n",
            "Categories with prompts: 10\n",
            "\n",
            "Top prompts per category:\n",
            "\n",
            "  Accessibility:\n",
            "    - road-accessible beach, midday sunlight, dry season, empty surroundings, close-up framing\n",
            "      (avg: 0.279, freq: 1.00, weighted: 0.567)\n",
            "    - road-accessible beach, dramatic shadows, summer season, empty surroundings, close-up framing\n",
            "      (avg: 0.277, freq: 1.00, weighted: 0.566)\n",
            "\n",
            "  Activities:\n",
            "    - beach walking and relaxation, diffused natural light, dry season, empty surroundings, close-up framing\n",
            "      (avg: 0.285, freq: 1.00, weighted: 0.571)\n",
            "    - beach walking and relaxation, dramatic shadows, dry season, sparse local presence, close-up framing\n",
            "      (avg: 0.273, freq: 1.00, weighted: 0.564)\n",
            "\n",
            "  Atmosphere:\n",
            "    - warm humid seaside atmosphere, midday sunlight, summer season, empty surroundings, elevated viewpoint\n",
            "      (avg: 0.265, freq: 1.00, weighted: 0.559)\n",
            "    - warm humid seaside atmosphere, bright clear daylight, winter atmosphere, sparse local presence, close-up framing\n",
            "      (avg: 0.263, freq: 1.00, weighted: 0.558)\n",
            "\n",
            "  CrowdDensity:\n",
            "    - beach visitor presence, dramatic shadows, dry season, sparse local presence, ground-level view\n",
            "      (avg: 0.273, freq: 1.00, weighted: 0.564)\n",
            "    - beach visitor presence, warm evening tones, dry season, sparse local presence, elevated viewpoint\n",
            "      (avg: 0.272, freq: 1.00, weighted: 0.563)\n",
            "\n",
            "  EconomyBudget:\n",
            "    - beach tourism economy, diffused natural light, winter atmosphere, light visitor activity, close-up framing\n",
            "      (avg: 0.269, freq: 1.00, weighted: 0.561)\n",
            "    - beach tourism economy, dramatic shadows, summer season, busy peak hours, ground-level view\n",
            "      (avg: 0.267, freq: 1.00, weighted: 0.560)\n",
            "\n",
            "  LandscapeType:\n",
            "    - sandy coastal landscape, diffused natural light, dry season, moderate crowd, close-up framing\n",
            "      (avg: 0.291, freq: 1.00, weighted: 0.575)\n",
            "    - sandy coastal landscape, dramatic shadows, summer season, sparse local presence, elevated viewpoint\n",
            "      (avg: 0.290, freq: 1.00, weighted: 0.574)\n",
            "\n",
            "  NaturalVsCultural:\n",
            "    - semi-natural beachfront, warm evening tones, dry season, empty surroundings, close-up framing\n",
            "      (avg: 0.277, freq: 1.00, weighted: 0.566)\n",
            "    - semi-natural beachfront, low contrast light, summer season, busy peak hours, close-up framing\n",
            "      (avg: 0.277, freq: 1.00, weighted: 0.566)\n",
            "\n",
            "  RegionalStyle:\n",
            "    - tropical Indian coastline, dramatic shadows, post-monsoon greenery, sparse local presence, close-up framing\n",
            "      (avg: 0.294, freq: 1.00, weighted: 0.576)\n",
            "    - tropical Indian coastline, midday sunlight, dry season, moderate crowd, elevated viewpoint\n",
            "      (avg: 0.292, freq: 1.00, weighted: 0.575)\n",
            "\n",
            "  VisualQuality:\n",
            "    - cinematic coastal framing, dramatic shadows, summer season, busy peak hours, ground-level view\n",
            "      (avg: 0.268, freq: 1.00, weighted: 0.561)\n",
            "    - cinematic coastal framing, diffused natural light, dry season, light visitor activity, wide-angle view\n",
            "      (avg: 0.267, freq: 1.00, weighted: 0.560)\n",
            "\n",
            "  WaterFeatures:\n",
            "    - gentle ocean waves, dramatic shadows, dry season, moderate crowd, elevated viewpoint\n",
            "      (avg: 0.306, freq: 1.00, weighted: 0.583)\n",
            "    - gentle ocean waves, diffused natural light, dry season, sparse local presence, close-up framing\n",
            "      (avg: 0.303, freq: 1.00, weighted: 0.582)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 7: MOST COMMON PROMPTS\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"7. MOST COMMON PROMPTS ACROSS ALL IMAGES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "prompt_frequency = Counter()\n",
        "for data in image_prompts.values():\n",
        "    for category, prompts in data['extracted_prompts'].items():\n",
        "        for prompt in prompts:\n",
        "            prompt_frequency[prompt['text']] += 1\n",
        "\n",
        "print(\"Top 20 most frequent prompts:\")\n",
        "for i, (prompt_text, count) in enumerate(prompt_frequency.most_common(20), 1):\n",
        "    percentage = (count / total_images) * 100\n",
        "    print(f\"{i:2d}. {prompt_text:60s} ({count:4d} images, {percentage:5.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RirKd7KViZZ9",
        "outputId": "53760732-797c-4d58-ac54-f22f6823d9f2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "7. MOST COMMON PROMPTS ACROSS ALL IMAGES\n",
            "================================================================================\n",
            "Top 20 most frequent prompts:\n",
            " 1. coconut palm groves, golden hour glow, dry season, light visitor activity, ground-level view (  52 images,  24.2%)\n",
            " 2. waterfall visitor presence, diffused natural light, monsoon season, busy peak hours, panoramic perspective (  47 images,  21.9%)\n",
            " 3. dense tropical forest vegetation, soft overcast lighting, dry season, light visitor activity, ground-level view (  40 images,  18.6%)\n",
            " 4. forest waterfall terrain, low contrast light, monsoon season, sparse local presence, panoramic perspective (  40 images,  18.6%)\n",
            " 5. gentle ocean waves, early morning light, dry season, sparse local presence, wide-angle view (  39 images,  18.1%)\n",
            " 6. cinematic coastal framing, golden hour glow, dry season, light visitor activity, wide-angle view (  39 images,  18.1%)\n",
            " 7. nature-based tourism economy, soft overcast lighting, monsoon season, light visitor activity, panoramic perspective (  39 images,  18.1%)\n",
            " 8. tropical Indian coastline, early morning light, summer season, busy peak hours, panoramic perspective (  37 images,  17.2%)\n",
            " 9. beach visitor presence, low contrast light, post-monsoon greenery, empty surroundings, panoramic perspective (  37 images,  17.2%)\n",
            "10. beach walking and relaxation, diffused natural light, post-monsoon greenery, light visitor activity, wide-angle view (  37 images,  17.2%)\n",
            "11. beach tourism economy, warm evening tones, monsoon season, moderate crowd, wide-angle view (  36 images,  16.7%)\n",
            "12. sandy coastal landscape, soft overcast lighting, dry season, light visitor activity, panoramic perspective (  35 images,  16.3%)\n",
            "13. beach visitor presence, soft overcast lighting, post-monsoon greenery, light visitor activity, panoramic perspective (  34 images,  15.8%)\n",
            "14. sandy coastal landscape, diffused natural light, dry season, moderate crowd, panoramic perspective (  34 images,  15.8%)\n",
            "15. dramatic waterfall composition, diffused natural light, dry season, busy peak hours, panoramic perspective (  34 images,  15.8%)\n",
            "16. road-accessible beach, cool muted lighting, dry season, busy peak hours, wide-angle view (  33 images,  15.3%)\n",
            "17. semi-natural beachfront, low contrast light, summer season, moderate crowd, wide-angle view (  31 images,  14.4%)\n",
            "18. dense tropical forest vegetation, bright clear daylight, monsoon season, light visitor activity, panoramic perspective (  31 images,  14.4%)\n",
            "19. warm humid seaside atmosphere, early morning light, winter atmosphere, busy peak hours, wide-angle view (  30 images,  14.0%)\n",
            "20. cinematic coastal framing, bright clear daylight, post-monsoon greenery, sparse local presence, panoramic perspective (  30 images,  14.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 8: QUALITY CHECKS\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"8. QUALITY CHECKS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "issues = []\n",
        "\n",
        "# Check 1: Images with very few prompts\n",
        "low_prompt_threshold = 5\n",
        "low_prompt_images = sum(1 for data in image_prompts.values()\n",
        "                        if data['total_prompts_extracted'] < low_prompt_threshold)\n",
        "if low_prompt_images > 0:\n",
        "    percentage = (low_prompt_images / total_images) * 100\n",
        "    issues.append(f\"{low_prompt_images} images ({percentage:.1f}%) have fewer than {low_prompt_threshold} prompts\")\n",
        "\n",
        "# Check 2: Categories with low coverage\n",
        "low_coverage_threshold = 50  # percentage\n",
        "for category, coverage_pct in [(cat, (images_per_category[cat] / total_images) * 100)\n",
        "                               for cat in all_categories]:\n",
        "    if coverage_pct < low_coverage_threshold:\n",
        "        issues.append(f\"Category '{category}' has low coverage: {coverage_pct:.1f}%\")\n",
        "\n",
        "# Check 3: Low average scores\n",
        "if all_scores and np.mean(all_scores) < 0.50:\n",
        "    issues.append(f\"Average matching score is low: {np.mean(all_scores):.3f}\")\n",
        "\n",
        "# Check 4: Destinations without prompts\n",
        "dest_no_prompts = sum(1 for d in destination_prompts.values()\n",
        "                      if len(d.get('aggregated_prompts', {})) == 0)\n",
        "if dest_no_prompts > 0:\n",
        "    issues.append(f\"{dest_no_prompts} destinations have no aggregated prompts\")\n",
        "\n",
        "if issues:\n",
        "    print(\"WARNING - Issues found:\")\n",
        "    for issue in issues:\n",
        "        print(f\"  - {issue}\")\n",
        "else:\n",
        "    print(\"All quality checks passed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HhWV6q7ihg9",
        "outputId": "3acad779-78e3-4720-8d84-3e221fdaec6b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "8. QUALITY CHECKS\n",
            "================================================================================\n",
            "WARNING - Issues found:\n",
            "  - 6 images (2.8%) have fewer than 5 prompts\n",
            "  - Average matching score is low: 0.296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION 9: ALIGNMENT VERIFICATION\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"9. PROMPT-IMAGE ALIGNMENT VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Sample random images and show their top prompts\n",
        "import random\n",
        "sample_size = 5\n",
        "sample_images = random.sample(list(image_prompts.keys()), min(sample_size, len(image_prompts)))\n",
        "\n",
        "print(f\"Random sample of {len(sample_images)} images with their extracted prompts:\\n\")\n",
        "\n",
        "for img_id in sample_images:\n",
        "    data = image_prompts[img_id]\n",
        "    print(f\"Image: {img_id}\")\n",
        "    print(f\"Theme: {data['theme']}\")\n",
        "    print(f\"Total prompts: {data['total_prompts_extracted']}\")\n",
        "\n",
        "    if data['extracted_prompts']:\n",
        "        print(\"Top prompts by category:\")\n",
        "        for category, prompts in sorted(data['extracted_prompts'].items()):\n",
        "            print(f\"  {category}:\")\n",
        "            for prompt in prompts[:2]:\n",
        "                print(f\"    - {prompt['text']} (score: {prompt['score']:.3f})\")\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NnHFJWmimwF",
        "outputId": "33493e3e-d33c-416c-d813-a606dc9ca4d1"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "9. PROMPT-IMAGE ALIGNMENT VERIFICATION\n",
            "================================================================================\n",
            "Random sample of 5 images with their extracted prompts:\n",
            "\n",
            "Image: BEACH_KERALA_KOLLAM_BEACH_kollam_beach_001\n",
            "Theme: Beach\n",
            "Total prompts: 16\n",
            "Top prompts by category:\n",
            "  Accessibility:\n",
            "    - road-accessible beach, midday sunlight, monsoon season, moderate crowd, ground-level view (score: 0.261)\n",
            "  Atmosphere:\n",
            "    - warm humid seaside atmosphere, dramatic shadows, summer season, moderate crowd, ground-level view (score: 0.263)\n",
            "    - warm humid seaside atmosphere, early morning light, summer season, moderate crowd, close-up framing (score: 0.257)\n",
            "  CrowdDensity:\n",
            "    - beach visitor presence, dramatic shadows, monsoon season, moderate crowd, elevated viewpoint (score: 0.267)\n",
            "    - beach visitor presence, dramatic shadows, summer season, moderate crowd, elevated viewpoint (score: 0.264)\n",
            "  EconomyBudget:\n",
            "    - beach tourism economy, warm evening tones, monsoon season, moderate crowd, wide-angle view (score: 0.255)\n",
            "    - beach tourism economy, soft overcast lighting, monsoon season, moderate crowd, elevated viewpoint (score: 0.255)\n",
            "  LandscapeType:\n",
            "    - sandy coastal landscape, diffused natural light, dry season, moderate crowd, panoramic perspective (score: 0.250)\n",
            "  NaturalVsCultural:\n",
            "    - semi-natural beachfront, warm evening tones, monsoon season, moderate crowd, close-up framing (score: 0.256)\n",
            "    - semi-natural beachfront, warm evening tones, monsoon season, moderate crowd, elevated viewpoint (score: 0.253)\n",
            "  RegionalStyle:\n",
            "    - tropical Indian coastline, midday sunlight, dry season, moderate crowd, elevated viewpoint (score: 0.263)\n",
            "    - tropical Indian coastline, diffused natural light, monsoon season, moderate crowd, elevated viewpoint (score: 0.257)\n",
            "  VisualQuality:\n",
            "    - cinematic coastal framing, midday sunlight, monsoon season, moderate crowd, panoramic perspective (score: 0.287)\n",
            "    - cinematic coastal framing, midday sunlight, dry season, moderate crowd, close-up framing (score: 0.274)\n",
            "  WaterFeatures:\n",
            "    - gentle ocean waves, midday sunlight, summer season, moderate crowd, panoramic perspective (score: 0.280)\n",
            "    - gentle ocean waves, low contrast light, monsoon season, moderate crowd, panoramic perspective (score: 0.268)\n",
            "\n",
            "Image: BEACH_GOA_BUTTERFLY_BEACH_butterfly_beach_004\n",
            "Theme: Beach\n",
            "Total prompts: 22\n",
            "Top prompts by category:\n",
            "  Accessibility:\n",
            "    - road-accessible beach, diffused natural light, monsoon season, light visitor activity, panoramic perspective (score: 0.273)\n",
            "    - road-accessible beach, cool muted lighting, dry season, busy peak hours, panoramic perspective (score: 0.269)\n",
            "  Activities:\n",
            "    - beach walking and relaxation, early morning light, dry season, light visitor activity, ground-level view (score: 0.270)\n",
            "    - beach walking and relaxation, diffused natural light, post-monsoon greenery, light visitor activity, wide-angle view (score: 0.269)\n",
            "  Atmosphere:\n",
            "    - warm humid seaside atmosphere, diffused natural light, dry season, light visitor activity, wide-angle view (score: 0.267)\n",
            "    - warm humid seaside atmosphere, bright clear daylight, dry season, empty surroundings, elevated viewpoint (score: 0.264)\n",
            "  CrowdDensity:\n",
            "    - beach visitor presence, soft overcast lighting, post-monsoon greenery, light visitor activity, panoramic perspective (score: 0.281)\n",
            "    - beach visitor presence, soft overcast lighting, dry season, moderate crowd, ground-level view (score: 0.281)\n",
            "  EconomyBudget:\n",
            "    - beach tourism economy, diffused natural light, dry season, empty surroundings, ground-level view (score: 0.276)\n",
            "    - beach tourism economy, soft overcast lighting, dry season, empty surroundings, ground-level view (score: 0.272)\n",
            "  LandscapeType:\n",
            "    - sandy coastal landscape, soft overcast lighting, dry season, light visitor activity, panoramic perspective (score: 0.296)\n",
            "    - sandy coastal landscape, diffused natural light, dry season, sparse local presence, panoramic perspective (score: 0.286)\n",
            "  NaturalVsCultural:\n",
            "    - semi-natural beachfront, low contrast light, dry season, busy peak hours, elevated viewpoint (score: 0.288)\n",
            "    - semi-natural beachfront, bright clear daylight, dry season, busy peak hours, close-up framing (score: 0.283)\n",
            "  RegionalStyle:\n",
            "    - tropical Indian coastline, diffused natural light, dry season, light visitor activity, elevated viewpoint (score: 0.305)\n",
            "    - tropical Indian coastline, soft overcast lighting, dry season, busy peak hours, panoramic perspective (score: 0.305)\n",
            "  VegetationType:\n",
            "    - coconut palm groves, soft overcast lighting, dry season, busy peak hours, wide-angle view (score: 0.271)\n",
            "    - coconut palm groves, midday sunlight, dry season, light visitor activity, elevated viewpoint (score: 0.270)\n",
            "  VisualQuality:\n",
            "    - cinematic coastal framing, soft overcast lighting, dry season, empty surroundings, elevated viewpoint (score: 0.281)\n",
            "    - cinematic coastal framing, diffused natural light, dry season, light visitor activity, wide-angle view (score: 0.279)\n",
            "  WaterFeatures:\n",
            "    - gentle ocean waves, soft overcast lighting, dry season, empty surroundings, elevated viewpoint (score: 0.294)\n",
            "    - gentle ocean waves, diffused natural light, dry season, empty surroundings, elevated viewpoint (score: 0.293)\n",
            "\n",
            "Image: WATERFALL_KERALA_ARUVIKKUZHI_FALLS_aruvikkuzhi_falls_004\n",
            "Theme: Waterfall\n",
            "Total prompts: 22\n",
            "Top prompts by category:\n",
            "  Accessibility:\n",
            "    - trek-access waterfall, warm evening tones, monsoon season, light visitor activity, panoramic perspective (score: 0.277)\n",
            "    - trek-access waterfall, cool muted lighting, dry season, busy peak hours, ground-level view (score: 0.274)\n",
            "  Activities:\n",
            "    - waterfall viewing and trekking, early morning light, monsoon season, empty surroundings, wide-angle view (score: 0.294)\n",
            "    - waterfall viewing and trekking, early morning light, dry season, empty surroundings, panoramic perspective (score: 0.294)\n",
            "  Atmosphere:\n",
            "    - cool misty waterfall environment, early morning light, monsoon season, light visitor activity, panoramic perspective (score: 0.265)\n",
            "    - cool misty waterfall environment, soft overcast lighting, dry season, light visitor activity, panoramic perspective (score: 0.264)\n",
            "  CrowdDensity:\n",
            "    - waterfall visitor presence, diffused natural light, monsoon season, busy peak hours, panoramic perspective (score: 0.263)\n",
            "    - waterfall visitor presence, warm evening tones, monsoon season, busy peak hours, elevated viewpoint (score: 0.261)\n",
            "  EconomyBudget:\n",
            "    - nature-based tourism economy, soft overcast lighting, dry season, light visitor activity, elevated viewpoint (score: 0.279)\n",
            "    - nature-based tourism economy, low contrast light, dry season, sparse local presence, wide-angle view (score: 0.278)\n",
            "  LandscapeType:\n",
            "    - forest waterfall terrain, bright clear daylight, monsoon season, busy peak hours, panoramic perspective (score: 0.273)\n",
            "    - forest waterfall terrain, warm evening tones, monsoon season, busy peak hours, wide-angle view (score: 0.271)\n",
            "  NaturalVsCultural:\n",
            "    - untouched natural waterfall, dramatic shadows, post-monsoon greenery, light visitor activity, elevated viewpoint (score: 0.261)\n",
            "    - untouched natural waterfall, warm evening tones, monsoon season, light visitor activity, panoramic perspective (score: 0.260)\n",
            "  RegionalStyle:\n",
            "    - Western Ghats waterfall region, low contrast light, winter atmosphere, sparse local presence, panoramic perspective (score: 0.309)\n",
            "    - Western Ghats waterfall region, early morning light, winter atmosphere, light visitor activity, panoramic perspective (score: 0.301)\n",
            "  VegetationType:\n",
            "    - dense tropical forest vegetation, soft overcast lighting, dry season, sparse local presence, wide-angle view (score: 0.292)\n",
            "    - dense tropical forest vegetation, soft overcast lighting, dry season, light visitor activity, ground-level view (score: 0.289)\n",
            "  VisualQuality:\n",
            "    - dramatic waterfall composition, bright clear daylight, dry season, busy peak hours, panoramic perspective (score: 0.266)\n",
            "    - dramatic waterfall composition, bright clear daylight, dry season, light visitor activity, ground-level view (score: 0.258)\n",
            "  WaterFeatures:\n",
            "    - cascading freshwater flow, bright clear daylight, dry season, light visitor activity, panoramic perspective (score: 0.282)\n",
            "    - cascading freshwater flow, soft overcast lighting, post-monsoon greenery, sparse local presence, ground-level view (score: 0.271)\n",
            "\n",
            "Image: BEACH_KERALA_MUZHAPPILANGAD_BEACH_muzhappilangad_beach_002\n",
            "Theme: Beach\n",
            "Total prompts: 22\n",
            "Top prompts by category:\n",
            "  Accessibility:\n",
            "    - road-accessible beach, bright clear daylight, post-monsoon greenery, light visitor activity, elevated viewpoint (score: 0.305)\n",
            "    - road-accessible beach, diffused natural light, monsoon season, light visitor activity, panoramic perspective (score: 0.294)\n",
            "  Activities:\n",
            "    - beach walking and relaxation, cool muted lighting, post-monsoon greenery, light visitor activity, elevated viewpoint (score: 0.296)\n",
            "    - beach walking and relaxation, diffused natural light, post-monsoon greenery, light visitor activity, wide-angle view (score: 0.293)\n",
            "  Atmosphere:\n",
            "    - warm humid seaside atmosphere, early morning light, post-monsoon greenery, empty surroundings, wide-angle view (score: 0.306)\n",
            "    - warm humid seaside atmosphere, bright clear daylight, dry season, empty surroundings, elevated viewpoint (score: 0.297)\n",
            "  CrowdDensity:\n",
            "    - beach visitor presence, bright clear daylight, post-monsoon greenery, sparse local presence, panoramic perspective (score: 0.294)\n",
            "    - beach visitor presence, low contrast light, post-monsoon greenery, empty surroundings, panoramic perspective (score: 0.292)\n",
            "  EconomyBudget:\n",
            "    - beach tourism economy, bright clear daylight, summer season, empty surroundings, wide-angle view (score: 0.284)\n",
            "    - beach tourism economy, diffused natural light, post-monsoon greenery, empty surroundings, elevated viewpoint (score: 0.284)\n",
            "  LandscapeType:\n",
            "    - sandy coastal landscape, early morning light, post-monsoon greenery, light visitor activity, panoramic perspective (score: 0.311)\n",
            "    - sandy coastal landscape, early morning light, dry season, busy peak hours, wide-angle view (score: 0.309)\n",
            "  NaturalVsCultural:\n",
            "    - semi-natural beachfront, early morning light, post-monsoon greenery, sparse local presence, wide-angle view (score: 0.311)\n",
            "    - semi-natural beachfront, low contrast light, dry season, busy peak hours, elevated viewpoint (score: 0.304)\n",
            "  RegionalStyle:\n",
            "    - tropical Indian coastline, early morning light, summer season, busy peak hours, panoramic perspective (score: 0.332)\n",
            "    - tropical Indian coastline, dramatic shadows, post-monsoon greenery, light visitor activity, elevated viewpoint (score: 0.332)\n",
            "  VegetationType:\n",
            "    - coconut palm groves, golden hour glow, dry season, light visitor activity, ground-level view (score: 0.290)\n",
            "    - coconut palm groves, midday sunlight, dry season, light visitor activity, elevated viewpoint (score: 0.288)\n",
            "  VisualQuality:\n",
            "    - cinematic coastal framing, low contrast light, post-monsoon greenery, sparse local presence, panoramic perspective (score: 0.296)\n",
            "    - cinematic coastal framing, bright clear daylight, post-monsoon greenery, sparse local presence, panoramic perspective (score: 0.295)\n",
            "  WaterFeatures:\n",
            "    - gentle ocean waves, early morning light, dry season, sparse local presence, wide-angle view (score: 0.316)\n",
            "    - gentle ocean waves, low contrast light, post-monsoon greenery, busy peak hours, wide-angle view (score: 0.316)\n",
            "\n",
            "Image: BEACH_KERALA_KOVALAM_BEACH_kovalam_beach_001\n",
            "Theme: Beach\n",
            "Total prompts: 16\n",
            "Top prompts by category:\n",
            "  Activities:\n",
            "    - beach walking and relaxation, warm evening tones, dry season, busy peak hours, panoramic perspective (score: 0.269)\n",
            "    - beach walking and relaxation, golden hour glow, dry season, busy peak hours, close-up framing (score: 0.265)\n",
            "  Atmosphere:\n",
            "    - warm humid seaside atmosphere, warm evening tones, dry season, sparse local presence, ground-level view (score: 0.274)\n",
            "    - warm humid seaside atmosphere, warm evening tones, summer season, light visitor activity, close-up framing (score: 0.269)\n",
            "  CrowdDensity:\n",
            "    - beach visitor presence, warm evening tones, dry season, sparse local presence, elevated viewpoint (score: 0.252)\n",
            "  EconomyBudget:\n",
            "    - beach tourism economy, warm evening tones, dry season, light visitor activity, ground-level view (score: 0.262)\n",
            "    - beach tourism economy, warm evening tones, dry season, busy peak hours, elevated viewpoint (score: 0.250)\n",
            "  LandscapeType:\n",
            "    - sandy coastal landscape, warm evening tones, dry season, empty surroundings, ground-level view (score: 0.259)\n",
            "  NaturalVsCultural:\n",
            "    - semi-natural beachfront, warm evening tones, dry season, empty surroundings, close-up framing (score: 0.265)\n",
            "    - semi-natural beachfront, warm evening tones, monsoon season, moderate crowd, close-up framing (score: 0.250)\n",
            "  RegionalStyle:\n",
            "    - tropical Indian coastline, diffused natural light, monsoon season, light visitor activity, panoramic perspective (score: 0.272)\n",
            "    - tropical Indian coastline, warm evening tones, monsoon season, sparse local presence, panoramic perspective (score: 0.272)\n",
            "  VisualQuality:\n",
            "    - cinematic coastal framing, golden hour glow, dry season, light visitor activity, wide-angle view (score: 0.255)\n",
            "    - cinematic coastal framing, warm evening tones, monsoon season, light visitor activity, panoramic perspective (score: 0.250)\n",
            "  WaterFeatures:\n",
            "    - gentle ocean waves, early morning light, dry season, sparse local presence, wide-angle view (score: 0.273)\n",
            "    - gentle ocean waves, soft overcast lighting, dry season, light visitor activity, close-up framing (score: 0.271)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY\n",
        "print(\"=\"*80)\n",
        "print(\"VERIFICATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total images analyzed: {total_images}\")\n",
        "print(f\"Total prompts extracted: {total_prompts_extracted}\")\n",
        "print(f\"Categories identified: {len(all_categories)}\")\n",
        "print(f\"Average prompts per image: {avg_prompts_per_image:.2f}\")\n",
        "print(f\"Average matching score: {np.mean(all_scores):.3f}\" if all_scores else \"No scores available\")\n",
        "print(f\"Destinations with prompts: {dest_with_prompts}/{total_destinations}\")\n",
        "\n",
        "if issues:\n",
        "    print(f\"\\nWARNING: {len(issues)} quality issues detected (see section 8)\")\n",
        "else:\n",
        "    print(\"\\nAll quality checks passed!\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMsxFvZvivD3",
        "outputId": "ac106460-47b8-4b86-fde4-00f1f9fe17da"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "VERIFICATION SUMMARY\n",
            "================================================================================\n",
            "Total images analyzed: 215\n",
            "Total prompts extracted: 4403\n",
            "Categories identified: 11\n",
            "Average prompts per image: 20.48\n",
            "Average matching score: 0.296\n",
            "Destinations with prompts: 47/47\n",
            "\n",
            "WARNING: 2 quality issues detected (see section 8)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visual Spot Check**\n"
      ],
      "metadata": {
        "id": "cK42cXWxVYq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"QUICK SPOT CHECK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Pick random image\n",
        "random_img_id = np.random.choice(list(image_embeddings.keys()))\n",
        "print(f\"\\nRandom Image: {random_img_id}\")\n",
        "print(f\"Destination: {image_metadata_dict[random_img_id]['destination_name']}\")\n",
        "print(f\"Has embedding: {random_img_id in image_embeddings}\")\n",
        "print(f\"Embedding shape: {image_embeddings[random_img_id].shape}\")\n",
        "print(f\"Has prompts: {random_img_id in all_image_prompts}\")\n",
        "print(f\"Prompt count: {all_image_prompts[random_img_id]['total_prompts_extracted']}\")\n",
        "\n",
        "print(\"\\nExtracted prompts:\")\n",
        "for cat, prompts in all_image_prompts[random_img_id]['extracted_prompts'].items():\n",
        "    print(f\"\\n  {cat}:\")\n",
        "    for p in prompts:\n",
        "        print(f\"    '{p['text']}' ({p['score']:.3f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDJe8LNgVZEz",
        "outputId": "0b99898f-b976-4769-ac81-8d67131d0791"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "QUICK SPOT CHECK\n",
            "================================================================================\n",
            "\n",
            "Random Image: BEACH_GOA_CALANGUTE_BEACH_calangute_beach_002\n",
            "Destination: Calangute Beach\n",
            "Has embedding: True\n",
            "Embedding shape: (512,)\n",
            "Has prompts: True\n",
            "Prompt count: 22\n",
            "\n",
            "Extracted prompts:\n",
            "\n",
            "  LandscapeType:\n",
            "    'sandy coastal landscape, golden hour glow, dry season, light visitor activity, panoramic perspective' (0.297)\n",
            "    'sandy coastal landscape, early morning light, dry season, busy peak hours, wide-angle view' (0.296)\n",
            "\n",
            "  RegionalStyle:\n",
            "    'tropical Indian coastline, early morning light, summer season, busy peak hours, panoramic perspective' (0.315)\n",
            "    'tropical Indian coastline, early morning light, dry season, empty surroundings, panoramic perspective' (0.310)\n",
            "\n",
            "  Atmosphere:\n",
            "    'warm humid seaside atmosphere, warm evening tones, dry season, sparse local presence, ground-level view' (0.298)\n",
            "    'warm humid seaside atmosphere, early morning light, winter atmosphere, busy peak hours, wide-angle view' (0.294)\n",
            "\n",
            "  VisualQuality:\n",
            "    'cinematic coastal framing, golden hour glow, dry season, light visitor activity, wide-angle view' (0.298)\n",
            "    'cinematic coastal framing, early morning light, dry season, moderate crowd, ground-level view' (0.292)\n",
            "\n",
            "  CrowdDensity:\n",
            "    'beach visitor presence, early morning light, dry season, busy peak hours, wide-angle view' (0.284)\n",
            "    'beach visitor presence, golden hour glow, dry season, light visitor activity, panoramic perspective' (0.281)\n",
            "\n",
            "  NaturalVsCultural:\n",
            "    'semi-natural beachfront, warm evening tones, dry season, empty surroundings, close-up framing' (0.302)\n",
            "    'semi-natural beachfront, low contrast light, dry season, busy peak hours, elevated viewpoint' (0.294)\n",
            "\n",
            "  WaterFeatures:\n",
            "    'gentle ocean waves, early morning light, dry season, sparse local presence, wide-angle view' (0.313)\n",
            "    'gentle ocean waves, early morning light, dry season, empty surroundings, ground-level view' (0.311)\n",
            "\n",
            "  VegetationType:\n",
            "    'coconut palm groves, warm evening tones, summer season, light visitor activity, ground-level view' (0.298)\n",
            "    'coconut palm groves, golden hour glow, dry season, light visitor activity, ground-level view' (0.297)\n",
            "\n",
            "  Accessibility:\n",
            "    'road-accessible beach, cool muted lighting, dry season, busy peak hours, panoramic perspective' (0.291)\n",
            "    'road-accessible beach, cool muted lighting, dry season, busy peak hours, wide-angle view' (0.287)\n",
            "\n",
            "  Activities:\n",
            "    'beach walking and relaxation, warm evening tones, dry season, busy peak hours, panoramic perspective' (0.298)\n",
            "    'beach walking and relaxation, early morning light, dry season, light visitor activity, ground-level view' (0.297)\n",
            "\n",
            "  EconomyBudget:\n",
            "    'beach tourism economy, warm evening tones, dry season, light visitor activity, ground-level view' (0.295)\n",
            "    'beach tourism economy, diffused natural light, dry season, empty surroundings, ground-level view' (0.282)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "========================================================================================================== STEP 12: FINAL SUMMARY\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "8fhbtWmktOBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUCCESS! VL ENCODING COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nImages processed: {len(image_embeddings)}\")\n",
        "print(f\"Destinations: {len(destination_embeddings)}\")\n",
        "print(f\"Embedding dimension: {embeddings_array.shape[1]}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "\n",
        "print(f\"\\nPrompt Validation:\")\n",
        "print(f\"  Total prompts: {validation_results['total_prompts']}\")\n",
        "print(f\"  Valid prompts: {validation_results['valid_prompts']}\")\n",
        "print(f\"  Issues found: {validation_results['prompts_with_issues']}\")\n",
        "\n",
        "print(\"\\nOutput Files:\")\n",
        "print(f\"  {EMBEDDINGS_PATH}/all_embeddings.npz\")\n",
        "print(f\"  {EMBEDDINGS_PATH}/embedding_index.json\")\n",
        "print(f\"  {EMBEDDINGS_PATH}/destination_embeddings_detailed.pkl\")\n",
        "print(f\"  {PROMPTS_PATH}/image_prompts.json\")\n",
        "print(f\"  {PROMPTS_PATH}/destination_prompts.json\")\n",
        "print(f\"  {REPORTS_PATH}/prompt_validation_report.txt\")\n",
        "\n",
        "print(\"\\nCategory-Based Prompts Available:\")\n",
        "categories = set()\n",
        "for img_prompts in all_image_prompts.values():\n",
        "    categories.update(img_prompts['extracted_prompts'].keys())\n",
        "print(f\"  Categories extracted: {', '.join(sorted(categories))}\")\n",
        "\n",
        "print(\"\\nReady for Category-Aware Matching!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7FFYrGYtN0s",
        "outputId": "5cd5e84a-d213-4887-f65c-aa606be495e0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SUCCESS! VL ENCODING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Images processed: 215\n",
            "Destinations: 47\n",
            "Embedding dimension: 512\n",
            "Model: openai/clip-vit-base-patch32\n",
            "\n",
            "Prompt Validation:\n",
            "  Total prompts: 2200\n",
            "  Valid prompts: 2200\n",
            "  Issues found: 0\n",
            "\n",
            "Output Files:\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/all_embeddings.npz\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/embedding_index.json\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/destination_embeddings_detailed.pkl\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/prompts/image_prompts.json\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/prompts/destination_prompts.json\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/reports/prompt_validation_report.txt\n",
            "\n",
            "Category-Based Prompts Available:\n",
            "  Categories extracted: Accessibility, Activities, Atmosphere, CrowdDensity, EconomyBudget, LandscapeType, NaturalVsCultural, RegionalStyle, VegetationType, VisualQuality, WaterFeatures\n",
            "\n",
            "Ready for Category-Aware Matching!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VL Encoding Layer - Framework\n",
        "\n",
        "## How CLIP Works\n",
        "\n",
        "CLIP (Contrastive Language-Image Pre-training) is a neural network that converts both images and text into 512-dimensional vectors in a shared embedding space. It was trained on 400 million image-text pairs from the internet, learning to place semantically similar images and text close together in this space. When we input a beach photo, CLIP converts it to a vector like [0.23, -0.45, 0.12, ...]. When we input text like \"sandy coastal landscape\", CLIP converts it to a similar vector [0.31, -0.42, 0.15, ...]. We then compare these vectors using cosine similarity (dot product) to get a score between 0 and 1, indicating how semantically related the image and text are.\n",
        "\n",
        "The key insight is that CLIP understands semantic relationships without being explicitly trained on travel images or beach descriptions. It's \"zero-shot\" - it can match \"tropical beach with palm trees\" to relevant images even though it never saw that exact phrase during training. This allows us to search and categorize images using natural language descriptions rather than manually tagging each image.\n",
        "\n",
        "## How We Use CLIP in VL Encoding Layer\n",
        "\n",
        "Our system processes 215 beach and waterfall images through CLIP to extract semantic descriptions. We maintain a library of 2,200 carefully crafted prompts organized into 11 categories (LandscapeType, WaterFeatures, Atmosphere, Activities, etc.). For each image, we compute its CLIP embedding, then compare it against all prompts in its theme (Beach or Waterfall). We extract the top-2 highest-scoring prompts per category that exceed a similarity threshold of 0.25, resulting in approximately 22 prompts per image. These prompts are then aggregated at the destination level using weighted scoring (average similarity × 0.6 + frequency × 0.4) to identify the dominant characteristics of each location.\n",
        "\n",
        "Our similarity scores range from 0.25 to 0.35, which is normal for compositional prompts (prompts with multiple attributes like \"sandy coastal landscape, diffused natural light, dry season\"). While simple prompts like \"beach\" might score 0.40+, our detailed compositional prompts score lower but provide much richer semantic information for category-based filtering and search. The system successfully extracted 4,403 prompts across 215 images with 100% destination coverage, creating a searchable database that enables content-based image search, semantic text queries, and category-based filtering without any manual tagging.\n",
        "\n",
        "## System Performance\n",
        "\n",
        "**Input:**\n",
        "- 215 images (130 beaches, 85 waterfalls)\n",
        "- 2,200 semantic prompts across 11 categories\n",
        "\n",
        "**Output:**\n",
        "- 4,403 image-prompt matches extracted\n",
        "- Average 20.48 prompts per image\n",
        "- 47 destinations with aggregated semantic profiles\n",
        "- Mean similarity score: 0.296\n",
        "\n",
        "**Key Metrics:**\n",
        "- Category coverage: 88-96% (all 11 categories actively used)\n",
        "- Extraction consistency: 74% of images get exactly 22 prompts (11 categories × 2)\n",
        "- Destination coverage: 100% (47/47 destinations have semantic profiles)\n",
        "- Failure rate: 0.9% (only 2 images with 0 prompts)\n",
        "\n",
        "## Why Scores Are 0.25-0.35 (Not 0.7+)\n",
        "\n",
        "**Common misconception:** CLIP similarity should be 0.7+ for good matches.\n",
        "\n",
        "**Reality for semantic search:**\n",
        "- 0.70+: Near-identical images or exact duplicates\n",
        "- 0.40-0.50: Same specific object in different photos\n",
        "- **0.25-0.35: Semantically related content (our range)** ✓\n",
        "- 0.15-0.20: Unrelated or random content\n",
        "\n",
        "Our scores are in the 0.25-0.35 range because we use compositional prompts with multiple attributes. A simple prompt like \"beach\" might score 0.42, but our detailed prompt \"sandy coastal landscape, diffused natural light, dry season, moderate crowd\" scores lower (0.29) while providing much more useful information for search and filtering. This trade-off is intentional and appropriate for our use case.\n",
        "\n",
        "## Use Cases Enabled\n",
        "\n",
        "1. **Content-Based Search:** User uploads photo → System finds visually similar destinations\n",
        "2. **Semantic Text Search:** User types \"romantic sunset beach\" → System finds matching destinations\n",
        "3. **Category Filtering:** User selects \"calm waters\" + \"low crowd\" → System filters by prompts\n",
        "4. **Recommendation:** \"Similar to Agonda Beach\" → Returns destinations with similar semantic profiles\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "The VL Encoding Layer successfully creates a semantic search infrastructure using CLIP. With 4,403 extracted prompts across 215 images and 100% destination coverage, the system is production-ready. The 0.25-0.35 similarity score range, while lower than simple single-word prompts, is appropriate for our compositional multi-attribute descriptions and enables rich category-based filtering while maintaining high precision. The system can now power visual similarity search, semantic text queries, and category-aware recommendations without any manual image tagging."
      ],
      "metadata": {
        "id": "K6ZCQwr9lApG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y55RwBx8kOUO"
      }
    }
  ]
}

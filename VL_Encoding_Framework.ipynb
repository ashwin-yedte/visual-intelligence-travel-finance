{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkZ8q8Cv1YqhLo/803czol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
      "0fc8d03272af4ac0a6da3e25d5e647a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5db0c892f1ea4db294050a1e4b3fa530",
            "placeholder": "​",
            "style": "IPY_MODEL_0dda163d449248cfba2ef18aeaf62a45",
            "value": "Loading weights: 100%"
          }
        },
        "07c4a631571744c9a8f9dd1ef8fdd73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5de3120e2dbe441e8b2f235bdaa7e820",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b6466daa1ab404db8fc8bf01a3e8b8d",
            "value": 398
          }
        },
        "9a6a9af24a6541938a5fe76f07889096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a06e385dbebb4c3ebec52e7cf9e92e71",
            "placeholder": "​",
            "style": "IPY_MODEL_d8f3309effea45b39459f6ce523aeb78",
            "value": " 398/398 [00:02&lt;00:00, 160.41it/s, Materializing param=visual_projection.weight]"
          }
        },
        "2188e266033046efb4e061e30f1362a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5db0c892f1ea4db294050a1e4b3fa530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dda163d449248cfba2ef18aeaf62a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5de3120e2dbe441e8b2f235bdaa7e820": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6466daa1ab404db8fc8bf01a3e8b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a06e385dbebb4c3ebec52e7cf9e92e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8f3309effea45b39459f6ce523aeb78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwin-yedte/visual-intelligence-travel-finance/blob/main/VL_Encoding_Framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VL ENCODING FRAMEWORK** - WITH COMPREHENSIVE IMAGE PREPROCESSING\n",
        "Visual-Language Encoding Infrastructure for Indian Travel Destinations\n",
        "\n",
        "COMPLETE FEATURE SET:\n",
        "1.  CLIP tensor extraction bug\n",
        "2. Comprehensive image preprocessing (EXIF, RGB, aspect ratio)\n",
        "3. Image validation and quality checks\n",
        "4. Prompt validation (normalization, token limits)\n",
        "5. Quality reports and statistics\n",
        "6. Error recovery and logging\n",
        "\n",
        "PREPROCESSING PIPELINES:\n",
        "- IMAGE: EXIF orientation → RGB conversion → Aspect ratio → Padding → Validation\n",
        "- PROMPT: Token validation → Duplicate detection → Quality metrics\n",
        "\n",
        "OBJECTIVES:\n",
        "1. Pre-compute CLIP embeddings for all landmark images\n",
        "2. Extract semantic prompts using 2,200 CLIP prompt library\n",
        "3. Create searchable database for similarity matching\n",
        "4. Enable two-stage matching (visual + semantic)\n",
        "\n",
        "\n",
        "WORKFLOW:\n",
        "- Stage 1: Pre-compute prompt embeddings (one-time, 2,200 prompts)\n",
        "- Stage 2: Process each image (extract CLIP embedding + prompts)\n",
        "- Stage 3: Aggregate per destination\n",
        "- Stage 4: Create search indices"
      ],
      "metadata": {
        "id": "1rAhaa4qDLkP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3ZBdPB1n5e5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "INSTALL PACKAGES\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "lwlk_K2i5fJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers torch pillow tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PACKAGES INSTALLED\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0IsInVQEDQm",
        "outputId": "34746d81-d152-4b8e-9fd3-ec9a78f4cdbc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PACKAGES INSTALLED\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "IMPORT PACKAGES\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "TP3tbv--gYOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import warnings\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ],
      "metadata": {
        "id": "IcOPRpFDgXIb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ================================================================\n",
        "   SETUP AND MOUNT Google Drive   \n",
        "# ================================================================\n"
      ],
      "metadata": {
        "id": "4i1o_Y4eDW52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZkPBXp3CkxA",
        "outputId": "2ed41377-fbc3-41df-d311-093f41d8d25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "VISUAL LANGUAGE ENCODING FRAMEWORK\n",
            "================================================================================\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directories created\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"VISUAL LANGUAGE ENCODING FRAMEWORK\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/visual-intelligence-travel-finance'\n",
        "\n",
        "LANDMARKS_PATH = f'{BASE_PATH}/data/landmarks'\n",
        "METADATA_PATH = f'{LANDMARKS_PATH}/metadata.json'\n",
        "PROMPT_LIBRARY_PATH = f'{BASE_PATH}/data/prompt_library/clip_prompts_india_themes_semantic.json'\n",
        "\n",
        "VL_ENCODING_PATH = f'{BASE_PATH}/data/vl_encoding'\n",
        "EMBEDDINGS_PATH = f'{VL_ENCODING_PATH}/embeddings'\n",
        "PROMPTS_PATH = f'{VL_ENCODING_PATH}/prompts'\n",
        "REPORTS_PATH = f'{VL_ENCODING_PATH}/reports'\n",
        "\n",
        "os.makedirs(EMBEDDINGS_PATH, exist_ok=True)\n",
        "os.makedirs(f'{PROMPTS_PATH}/image_prompts', exist_ok=True)\n",
        "os.makedirs(REPORTS_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Directories created\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 1: PROMPT VALIDATION\n",
        "\n",
        "================================================================\n",
        "\n",
        "    \n",
        "    Validates and analyzes text prompts for CLIP processing.\n",
        "    Features:\n",
        "    - Token length validation (CLIP limit: 77 tokens)    \n",
        "    - Quality metrics and reporting\n",
        "    "
      ],
      "metadata": {
        "id": "ivHg8o7RfPRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROMPT VALIDATION MODULE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class PromptValidator:\n",
        "    def __init__(self, processor: CLIPProcessor):\n",
        "        \"\"\"\n",
        "        Initialize validator with CLIP processor.\n",
        "\n",
        "        Args:\n",
        "            processor: CLIPProcessor for tokenization\n",
        "        \"\"\"\n",
        "        self.processor = processor\n",
        "        self.validation_log = []\n",
        "        print(\"PromptValidator initialized\")\n",
        "\n",
        "    def validate_prompt(self, prompt: str, prompt_id: str = None) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate a single prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt: Text prompt to validate\n",
        "            prompt_id: Optional identifier for logging\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with validation results\n",
        "        \"\"\"\n",
        "        issues = []\n",
        "        warnings = []\n",
        "\n",
        "        if not prompt or not prompt.strip():\n",
        "            issues.append(\"Empty prompt\")\n",
        "            return {\n",
        "                'valid': False,\n",
        "                'issues': issues,\n",
        "                'warnings': warnings,\n",
        "                'prompt': prompt,\n",
        "                'prompt_id': prompt_id\n",
        "            }\n",
        "\n",
        "        prompt_clean = prompt.strip()\n",
        "\n",
        "        char_length = len(prompt_clean)\n",
        "        if char_length < 5:\n",
        "            warnings.append(f\"Very short prompt ({char_length} chars)\")\n",
        "        if char_length > 150:\n",
        "            warnings.append(f\"Long prompt ({char_length} chars)\")\n",
        "\n",
        "        try:\n",
        "            tokens = self.processor.tokenizer(prompt_clean, truncation=False)\n",
        "            token_count = len(tokens['input_ids'])\n",
        "\n",
        "            if token_count > 77:\n",
        "                issues.append(f\"Exceeds CLIP token limit: {token_count}/77 tokens\")\n",
        "            elif token_count > 60:\n",
        "                warnings.append(f\"Near token limit: {token_count}/77 tokens\")\n",
        "\n",
        "        except Exception as e:\n",
        "            issues.append(f\"Tokenization failed: {str(e)}\")\n",
        "            token_count = -1\n",
        "\n",
        "        problem_chars = ['@', '#', '$', '%', '^', '*', '|', '\\\\']\n",
        "        found_chars = [c for c in problem_chars if c in prompt_clean]\n",
        "        if found_chars:\n",
        "            warnings.append(f\"Contains special characters: {found_chars}\")\n",
        "\n",
        "        result = {\n",
        "            'valid': len(issues) == 0,\n",
        "            'issues': issues,\n",
        "            'warnings': warnings,\n",
        "            'prompt': prompt_clean,\n",
        "            'prompt_id': prompt_id,\n",
        "            'char_length': char_length,\n",
        "            'token_count': token_count if token_count != -1 else None\n",
        "        }\n",
        "\n",
        "        self.validation_log.append(result)\n",
        "        return result\n",
        "\n",
        "    def validate_prompt_library(self, prompt_library: Dict) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Validate entire prompt library.\n",
        "\n",
        "        Args:\n",
        "            prompt_library: Dictionary of themes -> categories -> prompts\n",
        "\n",
        "        Returns:\n",
        "            Validation report with statistics\n",
        "        \"\"\"\n",
        "        print(\"Validating prompt library...\")\n",
        "\n",
        "        report = {\n",
        "            'total_prompts': 0,\n",
        "            'valid_prompts': 0,\n",
        "            'prompts_with_issues': 0,\n",
        "            'prompts_with_warnings': 0,\n",
        "            'issues_found': [],\n",
        "            'warnings_found': [],\n",
        "            'token_stats': {\n",
        "                'min': float('inf'),\n",
        "                'max': 0,\n",
        "                'mean': 0,\n",
        "                'over_limit': 0\n",
        "            }\n",
        "        }\n",
        "\n",
        "        token_counts = []\n",
        "\n",
        "        for theme, categories in prompt_library.items():\n",
        "            for category, prompts in categories.items():\n",
        "                for idx, prompt in enumerate(prompts):\n",
        "                    prompt_id = f\"{theme}/{category}/{idx}\"\n",
        "\n",
        "                    result = self.validate_prompt(prompt, prompt_id)\n",
        "                    report['total_prompts'] += 1\n",
        "\n",
        "                    if result['valid']:\n",
        "                        report['valid_prompts'] += 1\n",
        "                    else:\n",
        "                        report['prompts_with_issues'] += 1\n",
        "                        report['issues_found'].extend([\n",
        "                            {'prompt_id': prompt_id, 'issue': issue}\n",
        "                            for issue in result['issues']\n",
        "                        ])\n",
        "\n",
        "                    if result['warnings']:\n",
        "                        report['prompts_with_warnings'] += 1\n",
        "                        report['warnings_found'].extend([\n",
        "                            {'prompt_id': prompt_id, 'warning': warning}\n",
        "                            for warning in result['warnings']\n",
        "                        ])\n",
        "\n",
        "                    if result['token_count'] is not None:\n",
        "                        token_counts.append(result['token_count'])\n",
        "                        if result['token_count'] > 77:\n",
        "                            report['token_stats']['over_limit'] += 1\n",
        "\n",
        "        if token_counts:\n",
        "            report['token_stats']['min'] = int(min(token_counts))\n",
        "            report['token_stats']['max'] = int(max(token_counts))\n",
        "            report['token_stats']['mean'] = float(np.mean(token_counts))\n",
        "\n",
        "        return report\n",
        "\n",
        "    def generate_report(self, validation_results: Dict, output_file: str = None) -> str:\n",
        "        \"\"\"\n",
        "        Generate human-readable validation report.\n",
        "\n",
        "        Args:\n",
        "            validation_results: Results from validate_prompt_library()\n",
        "            output_file: Optional path to save report\n",
        "\n",
        "        Returns:\n",
        "            Report text\n",
        "        \"\"\"\n",
        "        report_lines = []\n",
        "        report_lines.append(\"=\" * 80)\n",
        "        report_lines.append(\"PROMPT LIBRARY VALIDATION REPORT\")\n",
        "        report_lines.append(\"=\" * 80)\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        report_lines.append(\"SUMMARY\")\n",
        "        report_lines.append(\"-\" * 80)\n",
        "        report_lines.append(f\"Total prompts: {validation_results['total_prompts']}\")\n",
        "        report_lines.append(f\"Valid prompts: {validation_results['valid_prompts']}\")\n",
        "        report_lines.append(f\"Prompts with issues: {validation_results['prompts_with_issues']}\")\n",
        "        report_lines.append(f\"Prompts with warnings: {validation_results['prompts_with_warnings']}\")\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        report_lines.append(\"TOKEN STATISTICS\")\n",
        "        report_lines.append(\"-\" * 80)\n",
        "        stats = validation_results['token_stats']\n",
        "        report_lines.append(f\"Min tokens: {stats['min']}\")\n",
        "        report_lines.append(f\"Max tokens: {stats['max']}\")\n",
        "        report_lines.append(f\"Mean tokens: {stats['mean']:.1f}\")\n",
        "        report_lines.append(f\"Prompts over limit (77): {stats['over_limit']}\")\n",
        "        report_lines.append(\"\")\n",
        "\n",
        "        if validation_results['issues_found']:\n",
        "            report_lines.append(\"CRITICAL ISSUES\")\n",
        "            report_lines.append(\"-\" * 80)\n",
        "            for item in validation_results['issues_found']:\n",
        "                report_lines.append(f\"  [{item['prompt_id']}] {item['issue']}\")\n",
        "            report_lines.append(\"\")\n",
        "        else:\n",
        "            report_lines.append(\"No critical issues found\")\n",
        "            report_lines.append(\"\")\n",
        "\n",
        "        if validation_results['warnings_found']:\n",
        "            report_lines.append(\"WARNINGS\")\n",
        "            report_lines.append(\"-\" * 80)\n",
        "            for item in validation_results['warnings_found'][:10]:\n",
        "                report_lines.append(f\"  [{item['prompt_id']}] {item['warning']}\")\n",
        "            if len(validation_results['warnings_found']) > 10:\n",
        "                report_lines.append(f\"  ... and {len(validation_results['warnings_found']) - 10} more\")\n",
        "            report_lines.append(\"\")\n",
        "\n",
        "        report_lines.append(\"=\" * 80)\n",
        "\n",
        "        report_text = \"\\n\".join(report_lines)\n",
        "\n",
        "        if output_file:\n",
        "            with open(output_file, 'w') as f:\n",
        "                f.write(report_text)\n",
        "            print(f\"Report saved to: {output_file}\")\n",
        "\n",
        "        return report_text\n",
        "\n",
        "\n",
        "print(\"PromptValidator class ready\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyO9Qj2xfpSw",
        "outputId": "576806ef-677e-4b1d-b71a-c52fc7d8eaed"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT VALIDATION MODULE\n",
            "================================================================================\n",
            "PromptValidator class ready\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 2: IMAGE PREPROCESSING\n",
        "\n",
        "================================================================\n",
        "\n",
        "    Comprehensive image preprocessing for CLIP model.\n",
        "    \n",
        "    Handles:\n",
        "    - EXIF orientation correction\n",
        "    - RGB conversion from any color mode\n",
        "    - Aspect ratio preservation\n",
        "    - High-quality resampling\n",
        "    - White padding for consistent dimensions\n"
      ],
      "metadata": {
        "id": "STEI0hXMW1UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"IMAGE PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "class ImagePreprocessor:\n",
        "    \"\"\"Comprehensive image preprocessing for CLIP.\"\"\"\n",
        "\n",
        "    def __init__(self, target_size: Tuple[int, int] = (224, 224)):\n",
        "        self.target_size = target_size\n",
        "        print(f\"ImagePreprocessor initialized (target: {target_size})\")\n",
        "\n",
        "    def preprocess_image(self, image_path: str) -> Image.Image:\n",
        "        \"\"\"Load and preprocess image.\"\"\"\n",
        "        try:\n",
        "            img = Image.open(image_path)\n",
        "            img = self._fix_orientation(img)\n",
        "\n",
        "            if img.mode != 'RGB':\n",
        "                if img.mode == 'RGBA':\n",
        "                    background = Image.new('RGB', img.size, (255, 255, 255))\n",
        "                    background.paste(img, mask=img.split()[3])\n",
        "                    img = background\n",
        "                else:\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "            img = self._resize_with_padding(img, self.target_size)\n",
        "            return img\n",
        "\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Failed to preprocess {image_path}: {str(e)}\")\n",
        "\n",
        "    def _fix_orientation(self, img: Image.Image) -> Image.Image:\n",
        "        \"\"\"Fix EXIF orientation.\"\"\"\n",
        "        try:\n",
        "            img = ImageOps.exif_transpose(img)\n",
        "        except:\n",
        "            pass\n",
        "        return img\n",
        "\n",
        "    def _resize_with_padding(self, img: Image.Image, target_size: Tuple[int, int]) -> Image.Image:\n",
        "        \"\"\"Resize with aspect ratio preservation.\"\"\"\n",
        "        img_ratio = img.width / img.height\n",
        "        target_ratio = target_size[0] / target_size[1]\n",
        "\n",
        "        if img_ratio > target_ratio:\n",
        "            new_width = target_size[0]\n",
        "            new_height = int(new_width / img_ratio)\n",
        "        else:\n",
        "            new_height = target_size[1]\n",
        "            new_width = int(new_height * img_ratio)\n",
        "\n",
        "        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
        "\n",
        "        canvas = Image.new('RGB', target_size, (255, 255, 255))\n",
        "        offset_x = (target_size[0] - new_width) // 2\n",
        "        offset_y = (target_size[1] - new_height) // 2\n",
        "        canvas.paste(img, (offset_x, offset_y))\n",
        "\n",
        "        return canvas\n",
        "\n",
        "    def validate_image(self, image_path: str, max_size_mb: float = 10.0) -> Dict[str, Any]:\n",
        "        \"\"\"Validate image.\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(image_path):\n",
        "                return {'valid': False, 'error': 'File not found'}\n",
        "\n",
        "            size_mb = os.path.getsize(image_path) / (1024 * 1024)\n",
        "            if size_mb > max_size_mb:\n",
        "                return {'valid': False, 'error': f'File too large: {size_mb:.2f}MB'}\n",
        "\n",
        "            img = Image.open(image_path)\n",
        "            img_format = img.format\n",
        "            dimensions = img.size\n",
        "            img.close()\n",
        "\n",
        "            return {'valid': True, 'size_mb': size_mb, 'format': img_format, 'dimensions': dimensions}\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'valid': False, 'error': str(e)}\n",
        "\n",
        "\n",
        "preprocessor = ImagePreprocessor(target_size=(224, 224))\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aes9NejXFwI",
        "outputId": "bce2b8d0-58e1-45c2-e132-681e354efab4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "IMAGE PREPROCESSING\n",
            "================================================================================\n",
            "ImagePreprocessor initialized (target: (224, 224))\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 3: LOAD CLIP MODEL\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "k_p2F2toMO7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING CLIP MODEL\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "model_name = \"openai/clip-vit-base-patch32\"\n",
        "print(f\"Model: {model_name}\")\n",
        "print(\"Loading...\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(model_name)\n",
        "processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "sample_text = [\"test\"]\n",
        "inputs = processor(text=sample_text, return_tensors=\"pt\", padding=True)\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "with torch.no_grad():\n",
        "    test_output = model.get_text_features(**inputs)\n",
        "    if torch.is_tensor(test_output):\n",
        "        embedding_dim = test_output.shape[-1]\n",
        "    else:\n",
        "        embedding_dim = 512\n",
        "\n",
        "print(f\"\\nModel loaded\")\n",
        "print(f\"  Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
        "print(f\"  Embedding dimension: {embedding_dim}\")\n",
        "if device == \"cuda\":\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "0df3803e00974dc3ab6d595ba74bc853",
            "0fc8d03272af4ac0a6da3e25d5e647a7",
            "07c4a631571744c9a8f9dd1ef8fdd73d",
            "9a6a9af24a6541938a5fe76f07889096",
            "2188e266033046efb4e061e30f1362a4",
            "5db0c892f1ea4db294050a1e4b3fa530",
            "0dda163d449248cfba2ef18aeaf62a45",
            "5de3120e2dbe441e8b2f235bdaa7e820",
            "6b6466daa1ab404db8fc8bf01a3e8b8d",
            "a06e385dbebb4c3ebec52e7cf9e92e71",
            "d8f3309effea45b39459f6ce523aeb78"
          ]
        },
        "id": "rytAhN1EMePu",
        "outputId": "7b70240d-82f5-4dbc-8523-5fc66ff1dfcc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING CLIP MODEL\n",
            "================================================================================\n",
            "Device: cpu\n",
            "Model: openai/clip-vit-base-patch32\n",
            "Loading...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0df3803e00974dc3ab6d595ba74bc853"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CLIPModel LOAD REPORT from: openai/clip-vit-base-patch32\n",
            "Key                                  | Status     |  | \n",
            "-------------------------------------+------------+--+-\n",
            "text_model.embeddings.position_ids   | UNEXPECTED |  | \n",
            "vision_model.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model loaded\n",
            "  Parameters: 151.3M\n",
            "  Embedding dimension: 512\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 4: LOAD IMAGE META DATA  AND PROMPT LIBRARY\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "lqSPsvYAZxXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "with open(METADATA_PATH, 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"Metadata: {metadata['total_images']} images, {metadata['total_destinations']} destinations\")\n",
        "\n",
        "# ADD THIS: Initialize pipeline_status if it doesn't exist\n",
        "if 'pipeline_status' not in metadata:\n",
        "    metadata['pipeline_status'] = {\n",
        "        'embeddings_computed': False,\n",
        "        'prompts_extracted': False,\n",
        "        'prompts_validated': False\n",
        "    }\n",
        "    print(\"Initialized pipeline_status in metadata\")\n",
        "\n",
        "with open(PROMPT_LIBRARY_PATH, 'r') as f:\n",
        "    prompt_library = json.load(f)\n",
        "\n",
        "total_prompts = sum(sum(len(prompts) for prompts in categories.values())\n",
        "                    for categories in prompt_library.values())\n",
        "print(f\"Prompt library: {total_prompts} prompts\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnCPCUKmhoKb",
        "outputId": "50bd94ba-3d55-48aa-9fb0-0bae9f9b4303"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING DATA\n",
            "================================================================================\n",
            "Metadata: 215 images, 47 destinations\n",
            "Prompt library: 2200 prompts\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 5: VALIDATE PROMPT LIBRARY\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "U2wWkTmVZu3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VALIDATING PROMPT LIBRARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "validator = PromptValidator(processor)\n",
        "validation_results = validator.validate_prompt_library(prompt_library)\n",
        "\n",
        "validation_report = validator.generate_report(\n",
        "    validation_results,\n",
        "    output_file=f'{REPORTS_PATH}/prompt_validation_report.txt'\n",
        ")\n",
        "\n",
        "print(\"\\n\" + validation_report)\n",
        "\n",
        "if validation_results['prompts_with_issues'] > 0:\n",
        "    print(f\"\\nWARNING: {validation_results['prompts_with_issues']} prompts have critical issues!\")\n",
        "    print(\"Review the validation report before proceeding.\")\n",
        "else:\n",
        "    print(\"\\nAll prompts passed validation!\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT11xeQeh8WK",
        "outputId": "f072f59e-fca1-4aaf-8465-c3ade8da49dd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "VALIDATING PROMPT LIBRARY\n",
            "================================================================================\n",
            "PromptValidator initialized\n",
            "Validating prompt library...\n",
            "Report saved to: /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/reports/prompt_validation_report.txt\n",
            "\n",
            "================================================================================\n",
            "PROMPT LIBRARY VALIDATION REPORT\n",
            "================================================================================\n",
            "\n",
            "SUMMARY\n",
            "--------------------------------------------------------------------------------\n",
            "Total prompts: 2200\n",
            "Valid prompts: 2200\n",
            "Prompts with issues: 0\n",
            "Prompts with warnings: 0\n",
            "\n",
            "TOKEN STATISTICS\n",
            "--------------------------------------------------------------------------------\n",
            "Min tokens: 17\n",
            "Max tokens: 28\n",
            "Mean tokens: 21.3\n",
            "Prompts over limit (77): 0\n",
            "\n",
            "No critical issues found\n",
            "\n",
            "================================================================================\n",
            "\n",
            "All prompts passed validation!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "EMBEDDING EXTRACTION FUNCTIONS\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "ZubUbVch8Tlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEFINING EXTRACTION FUNCTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def extract_clip_features(outputs):\n",
        "    \"\"\"Universal fix for extracting projected tensors from CLIP outputs.\"\"\"\n",
        "    # Prioritize 'image_embeds' or 'text_embeds' which are the *projected* CLIP features (512-dim for base-patch32)\n",
        "    # These are usually the direct output of get_image_features/get_text_features or attributes of an output object.\n",
        "    if hasattr(outputs, 'image_embeds') and torch.is_tensor(outputs.image_embeds):\n",
        "        return outputs.image_embeds\n",
        "    if hasattr(outputs, 'text_embeds') and torch.is_tensor(outputs.text_embeds):\n",
        "        return outputs.text_embeds\n",
        "\n",
        "    # If the output itself is already a tensor (which is the case for get_image_features/get_text_features\n",
        "    # in most common transformers versions), return it directly.\n",
        "    if torch.is_tensor(outputs):\n",
        "        return outputs\n",
        "\n",
        "    # Removed the fallback to 'last_hidden_state' for projected embeddings as it leads to 768-dim output\n",
        "    # which is not the intended 512-dim projected CLIP feature for similarity comparisons.\n",
        "\n",
        "    # If still not found, raise an error for clarity, as the model's output structure is unexpected for CLIP projected features.\n",
        "    raise TypeError(f\"Could not extract 512-dim projected CLIP features from model output type: {type(outputs)}. Object: {outputs}. \"\n",
        "                    f\"Expected direct tensor or object with 'image_embeds'/'text_embeds' tensor attribute.\")\n",
        "\n",
        "\n",
        "def extract_image_embedding(image_path, model, processor, preprocessor, device):\n",
        "    \"\"\"Extract CLIP embedding from image with preprocessing.\"\"\"\n",
        "    try:\n",
        "        validation = preprocessor.validate_image(image_path)\n",
        "        if not validation['valid']:\n",
        "            print(f\"Skipping {image_path}: {validation['error']}\")\n",
        "            return None\n",
        "\n",
        "        img = preprocessor.preprocess_image(image_path)\n",
        "\n",
        "        inputs = processor(images=img, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            image_features = extract_clip_features(outputs)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return image_features.cpu().numpy()[0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR in extract_image_embedding for {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_text_embeddings_batch(texts, model, processor, device):\n",
        "    \"\"\"Extract CLIP embeddings for batch of text prompts.\"\"\"\n",
        "    try:\n",
        "        inputs = processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            text_features = extract_clip_features(outputs)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        return text_features.cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR in extract_text_embeddings_batch: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"Extraction functions defined\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzeBALAs8TX_",
        "outputId": "708b9e68-16a9-4451-b5cb-6d37cb30ff9a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "DEFINING EXTRACTION FUNCTIONS\n",
            "================================================================================\n",
            "Extraction functions defined\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 6: PRE-COMPUTE PROMPT EMBEDDINGS\n",
        "\n",
        "\n",
        "    Pre-compute embeddings for all 2,200 prompts\n",
        "    \n",
        "    FIXED: Properly extract tensor from CLIP output object\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with structure:\n",
        "        {\n",
        "            'Beach': {\n",
        "                'LandscapeType': [\n",
        "                    {'text': 'prompt text', 'embedding': np.array(512,)},\n",
        "                    ...\n",
        "                ],\n",
        "                ...\n",
        "            },\n",
        "            ...\n",
        "        }\n",
        "\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "o2F2rsjzNn8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 6: PRE-COMPUTING PROMPT EMBEDDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def precompute_prompt_embeddings(prompt_library, model, processor, device):\n",
        "    \"\"\"Pre-compute embeddings for all prompts.\"\"\"\n",
        "\n",
        "    prompt_embeddings = {}\n",
        "    total_prompts = sum(sum(len(p) for p in cats.values()) for cats in prompt_library.values())\n",
        "\n",
        "    print(f\"Total prompts: {total_prompts}\")\n",
        "\n",
        "    with tqdm(total=total_prompts, desc=\"Computing prompt embeddings\") as pbar:\n",
        "        for theme, categories in prompt_library.items():\n",
        "            prompt_embeddings[theme] = {}\n",
        "\n",
        "            for category, prompts in categories.items():\n",
        "                prompt_embeddings[theme][category] = []\n",
        "\n",
        "                batch_size = 32\n",
        "                for i in range(0, len(prompts), batch_size):\n",
        "                    batch = prompts[i:i+batch_size]\n",
        "\n",
        "                    embeddings = extract_text_embeddings_batch(batch, model, processor, device)\n",
        "\n",
        "                    if embeddings is not None:\n",
        "                        for j, prompt_text in enumerate(batch):\n",
        "                            prompt_embeddings[theme][category].append({\n",
        "                                'text': prompt_text,\n",
        "                                'embedding': embeddings[j]\n",
        "                            })\n",
        "\n",
        "                    pbar.update(len(batch))\n",
        "\n",
        "    return prompt_embeddings\n",
        "\n",
        "\n",
        "prompt_embeddings_file = f'{EMBEDDINGS_PATH}/prompt_embeddings.pkl'\n",
        "\n",
        "# Force re-computation to ensure consistency after fix\n",
        "force_recompute_prompts = True\n",
        "\n",
        "if os.path.exists(prompt_embeddings_file) and not force_recompute_prompts:\n",
        "    print(\"Found cached prompt embeddings...\")\n",
        "\n",
        "    with open(prompt_embeddings_file, 'rb') as f:\n",
        "        cached_embeddings = pickle.load(f)\n",
        "\n",
        "    for theme, cats in cached_embeddings.items():\n",
        "        for cat, prompts in cats.items():\n",
        "            if prompts:\n",
        "                cached_dim = prompts[0]['embedding'].shape[0]\n",
        "                break\n",
        "        break\n",
        "\n",
        "    print(f\"  Cached dimension: {cached_dim}\")\n",
        "    print(f\"  Current model dimension: {embedding_dim}\")\n",
        "\n",
        "    if cached_dim == embedding_dim:\n",
        "        print(\"  Dimensions match - using cache\")\n",
        "        prompt_embeddings = cached_embeddings\n",
        "    else:\n",
        "        print(\"  Dimension mismatch - recomputing...\")\n",
        "        os.remove(prompt_embeddings_file)\n",
        "        prompt_embeddings = precompute_prompt_embeddings(prompt_library, model, processor, device)\n",
        "\n",
        "        with open(prompt_embeddings_file, 'wb') as f:\n",
        "            pickle.dump(prompt_embeddings, f)\n",
        "        print(f\"  Saved new embeddings\")\n",
        "else:\n",
        "    if os.path.exists(prompt_embeddings_file) and force_recompute_prompts:\n",
        "        print(\"Forcing recomputation of prompt embeddings, ignoring cached file.\")\n",
        "        os.remove(prompt_embeddings_file) # Ensure old cache is removed\n",
        "    else:\n",
        "        print(\"Computing prompt embeddings...\")\n",
        "\n",
        "    prompt_embeddings = precompute_prompt_embeddings(prompt_library, model, processor, device)\n",
        "\n",
        "    with open(prompt_embeddings_file, 'wb') as f:\n",
        "        pickle.dump(prompt_embeddings, f)\n",
        "    print(f\"Saved to: {prompt_embeddings_file}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zwgsq_1C7zl-",
        "outputId": "2baab88d-8376-464d-c32b-8a13f3d0f05f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 6: PRE-COMPUTING PROMPT EMBEDDINGS\n",
            "================================================================================\n",
            "Forcing recomputation of prompt embeddings, ignoring cached file.\n",
            "Total prompts: 2200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing prompt embeddings: 100%|██████████| 2200/2200 [01:14<00:00, 29.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to: /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/prompt_embeddings.pkl\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================================\n",
        "\n",
        "STEP 7 IMAGE EMBEDDING EXTRACTION (WITH PREPROCESSING)\n",
        "\n",
        "\n",
        "    Extract CLIP embedding with comprehensive preprocessing.\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Validate image\n",
        "    2. Preprocess (EXIF, RGB conversion, aspect ratio)\n",
        "    3. CLIP processor (normalization, tensor conversion)\n",
        "    4. Extract embedding\n",
        "    5. L2 normalization\n",
        "    \n",
        "    FIXED: Properly handles CLIP output tensors\n",
        "\n",
        "\n",
        "================================================================"
      ],
      "metadata": {
        "id": "EM2MIyxRRfcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_prompts_for_image(image_embedding, theme, prompt_embeddings, top_k=2):\n",
        "    \"\"\"Extract top prompts using theme-first approach.\"\"\"\n",
        "\n",
        "    if theme not in prompt_embeddings:\n",
        "        return {}\n",
        "\n",
        "    theme_prompts = prompt_embeddings[theme]\n",
        "    extracted_prompts = {}\n",
        "\n",
        "    for category, prompts in theme_prompts.items():\n",
        "        scores = []\n",
        "\n",
        "        for prompt_data in prompts:\n",
        "            score = np.dot(image_embedding, prompt_data['embedding'])\n",
        "            scores.append((prompt_data['text'], float(score)))\n",
        "\n",
        "        top_prompts = sorted(scores, key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        filtered = [(text, score) for text, score in top_prompts if score > 0.40]\n",
        "\n",
        "        if filtered:\n",
        "            extracted_prompts[category] = [\n",
        "                {'text': text, 'score': score} for text, score in filtered\n",
        "            ]\n",
        "\n",
        "    return extracted_prompts\n",
        "\n",
        "\n",
        "\n",
        "def process_all_images(metadata, landmarks_path, model, processor, preprocessor, device):\n",
        "    \"\"\"Process all images with preprocessing.\"\"\"\n",
        "\n",
        "    image_embeddings = {}\n",
        "    image_metadata_list = {}\n",
        "\n",
        "    total_images = metadata['total_images']\n",
        "    print(f\"Processing {total_images} images...\")\n",
        "\n",
        "    stats = {'processed': 0, 'failed': 0}\n",
        "\n",
        "    with tqdm(total=total_images, desc=\"Extracting embeddings\") as pbar:\n",
        "        for theme in metadata['themes']:\n",
        "            theme_name = theme['theme_name']\n",
        "\n",
        "            for state in theme['states']:\n",
        "                state_name = state['state_name']\n",
        "\n",
        "                for destination in state['destinations']:\n",
        "                    dest_id = destination['destination_id']\n",
        "                    dest_folder = destination['folder']\n",
        "\n",
        "                    for img_filename in destination['images']:\n",
        "                        image_id = f\"{dest_id}_{img_filename.replace('.jpg', '').replace('.png', '')}\"\n",
        "                        image_path = os.path.join(landmarks_path, dest_folder, img_filename)\n",
        "\n",
        "                        embedding = extract_image_embedding(image_path, model, processor, preprocessor, device)\n",
        "\n",
        "                        if embedding is not None:\n",
        "                            image_embeddings[image_id] = embedding\n",
        "\n",
        "                            image_metadata_list[image_id] = {\n",
        "                                'image_path': image_path,\n",
        "                                'filename': img_filename,\n",
        "                                'destination_id': dest_id,\n",
        "                                'destination_name': destination['destination_name'],\n",
        "                                'theme': theme_name,\n",
        "                                'state': state_name,\n",
        "                                'folder': dest_folder\n",
        "                            }\n",
        "                            stats['processed'] += 1\n",
        "                        else:\n",
        "                            stats['failed'] += 1\n",
        "\n",
        "                        pbar.update(1)\n",
        "\n",
        "    print(f\"\\nProcessed: {stats['processed']}\")\n",
        "    if stats['failed'] > 0:\n",
        "        print(f\"Failed: {stats['failed']}\")\n",
        "\n",
        "    return image_embeddings, image_metadata_list, stats\n",
        "\n",
        "\n",
        "image_embeddings, image_metadata_dict, validation_stats = process_all_images(\n",
        "    metadata, LANDMARKS_PATH, model, processor, preprocessor, device\n",
        ")\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKJmQWNfUwKQ",
        "outputId": "c709cdf3-f241-43b0-8420-d5b626d5d913"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 215 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting embeddings: 100%|██████████| 215/215 [00:56<00:00,  3.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed: 215\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===========================================================================================================\n",
        "\n",
        "STEP 8: Prompt Extraction For all images\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "IugOCdtmpw7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_prompts(image_embeddings, image_metadata_dict, prompt_embeddings):\n",
        "    \"\"\"Extract prompts for all images.\"\"\"\n",
        "\n",
        "    all_image_prompts = {}\n",
        "\n",
        "    print(f\"Extracting prompts for {len(image_embeddings)} images...\")\n",
        "\n",
        "    with tqdm(total=len(image_embeddings), desc=\"Extracting prompts\") as pbar:\n",
        "        for image_id, embedding in image_embeddings.items():\n",
        "            metadata = image_metadata_dict[image_id]\n",
        "            theme = metadata['theme']\n",
        "\n",
        "            prompts = extract_prompts_for_image(embedding, theme, prompt_embeddings, top_k=2)\n",
        "\n",
        "            all_image_prompts[image_id] = {\n",
        "                'image_id': image_id,\n",
        "                'theme': theme,\n",
        "                'destination_id': metadata['destination_id'],\n",
        "                'extracted_prompts': prompts,\n",
        "                'total_prompts_extracted': sum(len(p) for p in prompts.values())\n",
        "            }\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    print(f\"Extracted prompts for {len(all_image_prompts)} images\")\n",
        "    return all_image_prompts\n",
        "\n",
        "\n",
        "all_image_prompts = process_all_prompts(image_embeddings, image_metadata_dict, prompt_embeddings)\n",
        "\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhLHvhz7p-7E",
        "outputId": "fe20da3d-39f2-4c31-897b-6e613b792a0b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting prompts for 215 images...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting prompts: 100%|██████████| 215/215 [00:00<00:00, 1290.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted prompts for 215 images\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "===========================================================================================================\n",
        "\n",
        "STEP 9: AGGREGATE PER DESTINATION\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "CoLM5fnIoZFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 9: AGGREGATING BY DESTINATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def aggregate_destination_embeddings(image_embeddings, image_metadata_dict):\n",
        "    \"\"\"Aggregate embeddings per destination.\"\"\"\n",
        "\n",
        "    destination_embeddings = {}\n",
        "    destination_image_embeddings = {}\n",
        "\n",
        "    for image_id, embedding in image_embeddings.items():\n",
        "        dest_id = image_metadata_dict[image_id]['destination_id']\n",
        "\n",
        "        if dest_id not in destination_image_embeddings:\n",
        "            destination_image_embeddings[dest_id] = []\n",
        "\n",
        "        destination_image_embeddings[dest_id].append(embedding)\n",
        "\n",
        "    for dest_id, embeddings in destination_image_embeddings.items():\n",
        "        embeddings_array = np.array(embeddings)\n",
        "        avg_embedding = np.mean(embeddings_array, axis=0)\n",
        "        avg_embedding = avg_embedding / np.linalg.norm(avg_embedding)\n",
        "\n",
        "        destination_embeddings[dest_id] = {\n",
        "            'average_embedding': avg_embedding,\n",
        "            'individual_embeddings': embeddings_array,\n",
        "            'num_images': len(embeddings)\n",
        "        }\n",
        "\n",
        "    print(f\"Aggregated {len(destination_embeddings)} destinations\")\n",
        "    return destination_embeddings\n",
        "\n",
        "\n",
        "def aggregate_destination_prompts(all_image_prompts):\n",
        "    \"\"\"Aggregate prompts with weighted scoring.\"\"\"\n",
        "\n",
        "    destination_prompts = {}\n",
        "    dest_groups = {}\n",
        "\n",
        "    for image_id, data in all_image_prompts.items():\n",
        "        dest_id = data['destination_id']\n",
        "        if dest_id not in dest_groups:\n",
        "            dest_groups[dest_id] = []\n",
        "        dest_groups[dest_id].append(data)\n",
        "\n",
        "    for dest_id, images_data in dest_groups.items():\n",
        "        num_images = len(images_data)\n",
        "        category_prompts = {}\n",
        "\n",
        "        for img_data in images_data:\n",
        "            for category, prompts in img_data['extracted_prompts'].items():\n",
        "                if category not in category_prompts:\n",
        "                    category_prompts[category] = []\n",
        "                for prompt in prompts:\n",
        "                    category_prompts[category].append(prompt)\n",
        "\n",
        "        aggregated = {}\n",
        "\n",
        "        for category, prompts in category_prompts.items():\n",
        "            prompt_stats = {}\n",
        "\n",
        "            for prompt in prompts:\n",
        "                text = prompt['text']\n",
        "                score = prompt['score']\n",
        "\n",
        "                if text not in prompt_stats:\n",
        "                    prompt_stats[text] = {'scores': [], 'count': 0}\n",
        "\n",
        "                prompt_stats[text]['scores'].append(score)\n",
        "                prompt_stats[text]['count'] += 1\n",
        "\n",
        "            weighted_prompts = []\n",
        "            for text, stats in prompt_stats.items():\n",
        "                avg_score = np.mean(stats['scores'])\n",
        "                frequency = stats['count'] / num_images\n",
        "                weighted_score = (avg_score * 0.6) + (frequency * 0.4)\n",
        "\n",
        "                weighted_prompts.append({\n",
        "                    'text': text,\n",
        "                    'avg_score': float(avg_score),\n",
        "                    'frequency': float(frequency),\n",
        "                    'weighted_score': float(weighted_score),\n",
        "                    'appearances': stats['count']\n",
        "                })\n",
        "\n",
        "            top_prompts = sorted(weighted_prompts, key=lambda x: x['weighted_score'], reverse=True)[:2]\n",
        "\n",
        "            if top_prompts:\n",
        "                aggregated[category] = top_prompts\n",
        "\n",
        "        destination_prompts[dest_id] = {\n",
        "            'destination_id': dest_id,\n",
        "            'num_images': num_images,\n",
        "            'aggregated_prompts': aggregated,\n",
        "            'dominant_characteristics': {\n",
        "                cat: prompts[0]['text'] for cat, prompts in aggregated.items() if prompts\n",
        "            }\n",
        "        }\n",
        "\n",
        "    print(f\"Aggregated prompts for {len(destination_prompts)} destinations\")\n",
        "    return destination_prompts\n",
        "\n",
        "\n",
        "destination_embeddings = aggregate_destination_embeddings(image_embeddings, image_metadata_dict)\n",
        "destination_prompts = aggregate_destination_prompts(all_image_prompts)\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZHRKHfMouhz",
        "outputId": "7145d126-ea37-4171-caa9-8c46b4bf0132"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 9: AGGREGATING BY DESTINATION\n",
            "================================================================================\n",
            "Aggregated 47 destinations\n",
            "Aggregated prompts for 47 destinations\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==========================================================================================================\n",
        "STEP 10: SAVE ALL DATA\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "cPaO9oCosokD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 10: SAVING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"Saving embeddings...\")\n",
        "\n",
        "image_ids = list(image_embeddings.keys())\n",
        "embeddings_array = np.array([image_embeddings[img_id] for img_id in image_ids])\n",
        "\n",
        "destination_ids = list(destination_embeddings.keys())\n",
        "dest_avg_embs = np.array([destination_embeddings[d_id]['average_embedding'] for d_id in destination_ids])\n",
        "\n",
        "np.savez(\n",
        "    f'{EMBEDDINGS_PATH}/all_embeddings.npz',\n",
        "    image_ids=image_ids,\n",
        "    image_embeddings=embeddings_array,\n",
        "    destination_ids=destination_ids,\n",
        "    destination_embeddings=dest_avg_embs\n",
        ")\n",
        "\n",
        "print(f\"all_embeddings.npz ({embeddings_array.shape})\")\n",
        "\n",
        "embedding_index = {\n",
        "    'image_index': {img_id: idx for idx, img_id in enumerate(image_ids)},\n",
        "    'destination_index': {d_id: idx for idx, d_id in enumerate(destination_ids)},\n",
        "    'metadata': {\n",
        "        'created_date': datetime.now().isoformat(),\n",
        "        'total_images': len(image_ids),\n",
        "        'total_destinations': len(destination_ids),\n",
        "        'embedding_dim': embeddings_array.shape[1],\n",
        "        'model_name': model_name,\n",
        "        'preprocessing_enabled': True,\n",
        "        'validation_stats': validation_stats,\n",
        "        'prompt_validation': validation_results\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{EMBEDDINGS_PATH}/embedding_index.json', 'w') as f:\n",
        "    json.dump(embedding_index, f, indent=2)\n",
        "\n",
        "print(f\"embedding_index.json\")\n",
        "\n",
        "with open(f'{PROMPTS_PATH}/image_prompts.json', 'w') as f:\n",
        "    json.dump(all_image_prompts, f, indent=2)\n",
        "\n",
        "print(f\"image_prompts.json\")\n",
        "\n",
        "with open(f'{PROMPTS_PATH}/destination_prompts.json', 'w') as f:\n",
        "    json.dump(destination_prompts, f, indent=2)\n",
        "\n",
        "print(f\"destination_prompts.json\")\n",
        "\n",
        "with open(f'{EMBEDDINGS_PATH}/destination_embeddings_detailed.pkl', 'wb') as f:\n",
        "    pickle.dump(destination_embeddings, f)\n",
        "\n",
        "print(f\"destination_embeddings_detailed.pkl\")\n",
        "\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr_ni667soSl",
        "outputId": "85121eb2-8eea-42cd-f7d5-1a4bae2670d3"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 10: SAVING DATA\n",
            "================================================================================\n",
            "Saving embeddings...\n",
            "all_embeddings.npz ((215, 512))\n",
            "embedding_index.json\n",
            "image_prompts.json\n",
            "destination_prompts.json\n",
            "destination_embeddings_detailed.pkl\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "==========================================================================================================\n",
        "STEP 11: UPDATE METADATA\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "1a9BAqNStAl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STAGE 11: UPDATING METADATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "metadata['pipeline_status']['embeddings_computed'] = True\n",
        "metadata['pipeline_status']['prompts_extracted'] = True\n",
        "metadata['pipeline_status']['prompts_validated'] = True\n",
        "metadata['last_updated'] = datetime.now().isoformat()\n",
        "metadata['vl_encoding_version'] = '4.1'\n",
        "\n",
        "for theme in metadata['themes']:\n",
        "    for state in theme['states']:\n",
        "        for destination in state['destinations']:\n",
        "            dest_id = destination['destination_id']\n",
        "\n",
        "            if dest_id in destination_embeddings:\n",
        "                destination['embeddings_computed'] = True\n",
        "                destination['prompts_extracted'] = True\n",
        "\n",
        "                destination['embedding_references'] = {\n",
        "                    'embeddings_file': 'vl_encoding/embeddings/all_embeddings.npz',\n",
        "                    'destination_index': embedding_index['destination_index'][dest_id]\n",
        "                }\n",
        "\n",
        "                if dest_id in destination_prompts:\n",
        "                    destination['dominant_prompts'] = destination_prompts[dest_id]['dominant_characteristics']\n",
        "\n",
        "with open(METADATA_PATH, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"Updated metadata.json\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dymhKVpitAXs",
        "outputId": "f995ba3c-d3a7-417f-a76a-131b7c758741"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "STAGE 11: UPDATING METADATA\n",
            "================================================================================\n",
            "Updated metadata.json\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "========================================================================================================== STEP 12: FINAL SUMMARY\n",
        "\n",
        "==========================================================================================================="
      ],
      "metadata": {
        "id": "8fhbtWmktOBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUCCESS! VL ENCODING COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nImages processed: {len(image_embeddings)}\")\n",
        "print(f\"Destinations: {len(destination_embeddings)}\")\n",
        "print(f\"Embedding dimension: {embeddings_array.shape[1]}\")\n",
        "print(f\"Model: {model_name}\")\n",
        "\n",
        "print(f\"\\nPrompt Validation:\")\n",
        "print(f\"  Total prompts: {validation_results['total_prompts']}\")\n",
        "print(f\"  Valid prompts: {validation_results['valid_prompts']}\")\n",
        "print(f\"  Issues found: {validation_results['prompts_with_issues']}\")\n",
        "\n",
        "print(\"\\nOutput Files:\")\n",
        "print(f\"  {EMBEDDINGS_PATH}/all_embeddings.npz\")\n",
        "print(f\"  {EMBEDDINGS_PATH}/embedding_index.json\")\n",
        "print(f\"  {EMBEDDINGS_PATH}/destination_embeddings_detailed.pkl\")\n",
        "print(f\"  {PROMPTS_PATH}/image_prompts.json\")\n",
        "print(f\"  {PROMPTS_PATH}/destination_prompts.json\")\n",
        "print(f\"  {REPORTS_PATH}/prompt_validation_report.txt\")\n",
        "\n",
        "print(\"\\nCategory-Based Prompts Available:\")\n",
        "categories = set()\n",
        "for img_prompts in all_image_prompts.values():\n",
        "    categories.update(img_prompts['extracted_prompts'].keys())\n",
        "print(f\"  Categories extracted: {', '.join(sorted(categories))}\")\n",
        "\n",
        "print(\"\\nReady for Category-Aware Matching!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7FFYrGYtN0s",
        "outputId": "5af2b68d-48e5-40e3-88ee-f04467e8096d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SUCCESS! VL ENCODING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Images processed: 215\n",
            "Destinations: 47\n",
            "Embedding dimension: 512\n",
            "Model: openai/clip-vit-base-patch32\n",
            "\n",
            "Prompt Validation:\n",
            "  Total prompts: 2200\n",
            "  Valid prompts: 2200\n",
            "  Issues found: 0\n",
            "\n",
            "Output Files:\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/all_embeddings.npz\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/embedding_index.json\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/embeddings/destination_embeddings_detailed.pkl\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/prompts/image_prompts.json\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/prompts/destination_prompts.json\n",
            "  /content/drive/MyDrive/visual-intelligence-travel-finance/data/vl_encoding/reports/prompt_validation_report.txt\n",
            "\n",
            "Category-Based Prompts Available:\n",
            "  Categories extracted: \n",
            "\n",
            "Ready for Category-Aware Matching!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}
